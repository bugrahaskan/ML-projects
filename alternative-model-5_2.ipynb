{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137ffa88-7b70-4dab-8f65-5831a80d4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cddf6b-bde8-414f-9bfe-7e3e3105d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = data.iloc[3524:]\n",
    "data = data.iloc[:3524]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc97307-76c1-4b28-9dcb-0264e05a5d3a",
   "metadata": {},
   "source": [
    "### model5 for different time steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d15f5-834e-4f59-a4f4-3358c38832fd",
   "metadata": {},
   "source": [
    "#### 10 bar lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261fe3ba-abbf-422a-ba21-d4f75892189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition_high = seasonal_decompose(data['high'], model='additive', period=30)\n",
    "trend_high = decomposition_high.trend\n",
    "\n",
    "decomposition_low = seasonal_decompose(data['low'], model='additive', period=30)\n",
    "trend_low = decomposition_low.trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c14d5e9-51ac-4649-b5ae-5a9cb73b08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trend_high'] = trend_high\n",
    "data['trend_low'] = trend_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5794097-3789-4fb0-bcb1-b0a5f7d86262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in range(1,11):\n",
    "    data[f'trend_h+{i}'] = data['trend_high'].shift(i)\n",
    "for i in range(1,11):\n",
    "    data[f'trend_l+{i}'] = data['trend_low'].shift(i)\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8261b7-a415-43c3-b208-186d4f6e4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_high = data.iloc[:, [8,9,10,11,12,13,14,15,16,17]]\n",
    "target_high = data.iloc[:, [6]]\n",
    "\n",
    "inputs_low = data.iloc[:, [18,19,20,21,22,23,24,25,26,27]]\n",
    "target_low = data.iloc[:, [7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df25d6c1-5dc0-4011-8521-21061ff36660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(inputs_high, target_high, test_size=0.2, random_state=1, shuffle=False)\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(inputs_low, target_low, test_size=0.2, random_state=1, shuffle=False)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler_h = MinMaxScaler()\n",
    "scaler_h.fit(X_train_h)\n",
    "scaler_target_h = MinMaxScaler()\n",
    "scaler_target_h.fit(y_train_h)\n",
    "\n",
    "scaler_l = MinMaxScaler()\n",
    "scaler_l.fit(X_train_l)\n",
    "scaler_target_l = MinMaxScaler()\n",
    "scaler_target_l.fit(y_train_l)\n",
    "\n",
    "X_train_h_scaled = scaler_h.transform(X_train_h)\n",
    "X_test_h_scaled = scaler_h.transform(X_test_h)\n",
    "y_train_h_scaled = scaler_target_h.transform(y_train_h)\n",
    "y_test_h_scaled = scaler_target_h.transform(y_test_h)\n",
    "\n",
    "X_train_l_scaled = scaler_l.transform(X_train_l)\n",
    "X_test_l_scaled = scaler_l.transform(X_test_l)\n",
    "y_train_l_scaled = scaler_target_l.transform(y_train_l)\n",
    "y_test_l_scaled = scaler_target_l.transform(y_test_l)\n",
    "\n",
    "random_tree_high_10 = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "random_tree_high_10.fit(X_train_h_scaled, y_train_h_scaled)\n",
    "\n",
    "random_tree_low_10 = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "random_tree_low_10.fit(X_train_l_scaled, y_train_l_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a7681-b659-4b43-8eb4-a9519d8bed78",
   "metadata": {},
   "source": [
    "#### data reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c4fc0e-f2bd-4497-9b93-548ccd8beac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data_backup.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d882e2fe-c1e6-4853-918f-12a74c01ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition_high_backup = seasonal_decompose(data_backup['high'], model='additive', period=30)\n",
    "trend_high_backup = decomposition_high_backup.trend\n",
    "\n",
    "decomposition_low_backup = seasonal_decompose(data_backup['low'], model='additive', period=30)\n",
    "trend_low_backup = decomposition_low_backup.trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5dc67b-cfe6-46c3-b7b9-e203b7a637ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup['trend_high'] = trend_high_backup\n",
    "data_backup['trend_low'] = trend_low_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65a28d34-0d50-4f57-983e-857b87cb4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in range(1,11):\n",
    "    data_backup[f'trend_h+{i}'] = data_backup['trend_high'].shift(i)\n",
    "for i in range(1,11):\n",
    "    data_backup[f'trend_l+{i}'] = data_backup['trend_low'].shift(i)\n",
    "data_backup.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6730b4-433e-4273-b647-0ac7cd9d6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_high_backup = data_backup.iloc[:, [8,9,10,11,12,13,14,15,16,17]]\n",
    "inputs_low_backup = data_backup.iloc[:, [18,19,20,21,22,23,24,25,26,27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5730eeb9-f91e-473f-b141-ee8ddf63e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = dict()\n",
    "\n",
    "scaler_input = MinMaxScaler()\n",
    "data_backup_h_scaled = scaler_input.fit_transform(inputs_high_backup)\n",
    "scaler_target = MinMaxScaler()\n",
    "target_backup_c_scaled = scaler_target.fit_transform(data_backup[['trend_high']])\n",
    "\n",
    "y_pred_h_backup = random_tree_high_10.predict(data_backup_h_scaled)\n",
    "y_pred_h_backup = y_pred_h_backup.reshape(-1,1)\n",
    "y_pred_h_backup = scaler_target.inverse_transform(y_pred_h_backup)\n",
    "\n",
    "scaler_input = MinMaxScaler()\n",
    "data_backup_l_scaled = scaler_input.fit_transform(inputs_low_backup)\n",
    "scaler_target = MinMaxScaler()\n",
    "target_backup_c_scaled = scaler_target.fit_transform(data_backup[['trend_low']])\n",
    "\n",
    "y_pred_l_backup = random_tree_low_10.predict(data_backup_l_scaled)\n",
    "y_pred_l_backup = y_pred_l_backup.reshape(-1,1)\n",
    "y_pred_l_backup = scaler_target.inverse_transform(y_pred_l_backup)\n",
    "\n",
    "df_pred = {'pred_h':y_pred_h_backup, 'pred_l':y_pred_l_backup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a3b276-7fd1-4b09-87a4-192963872afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS FOR NASDAQ INDEX:\n",
      "score high: 81.8052708329642\n",
      "score low: 89.86792941464195\n",
      "- R2 METRICS FOR NASDAQ INDEX:\n",
      "r2 high: 0.9987990885885053\n",
      "r2 low: 0.9984928620114331\n"
     ]
    }
   ],
   "source": [
    "data_backup.reset_index(inplace=True)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score, mean_absolute_error\n",
    "\n",
    "score_h = mean_absolute_error(data_backup[['high']], df_pred['pred_h'])\n",
    "r2_h = r2_score(data_backup[['high']], df_pred['pred_h'])\n",
    "data_backup['mae_h'] = score_h\n",
    "\n",
    "score_l = mean_absolute_error(data_backup[['low']], df_pred['pred_l'])\n",
    "r2_l = r2_score(data_backup[['low']], df_pred['pred_l'])\n",
    "data_backup['mae_l'] = score_l\n",
    "\n",
    "print(f'METRICS FOR NASDAQ INDEX:')\n",
    "print(f'score high: {score_h}')\n",
    "print(f'score low: {score_l}')\n",
    "\n",
    "print(f'- R2 METRICS FOR NASDAQ INDEX:')\n",
    "print(f'r2 high: {r2_h}')\n",
    "print(f'r2 low: {r2_l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ce85ea-d89c-42c3-a49d-5641353df482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148943/2133779364.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_backup['pred_high'] = y_pred_h_series\n",
      "/tmp/ipykernel_148943/2133779364.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_backup['pred_low'] = y_pred_l_series\n"
     ]
    }
   ],
   "source": [
    "remaining_columns = ('date','open','high','low','close','volume','trend_high','trend_low','mae_h','mae_l')\n",
    "\n",
    "data_backup = data_backup[list(remaining_columns)]\n",
    "\n",
    "y_pred_h_series = pd.Series(df_pred['pred_h'].reshape(1,-1)[0])\n",
    "y_pred_l_series = pd.Series(df_pred['pred_l'].reshape(1,-1)[0])\n",
    "data_backup['pred_high'] = y_pred_h_series\n",
    "data_backup['pred_low'] = y_pred_l_series\n",
    "\n",
    "data_backup.to_excel(f'data_w_pred_m5/index_w_pred_m5_10bar.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88703c2a-6815-43a9-9e47-fe5adcac4fe4",
   "metadata": {},
   "source": [
    "#### 30 bar lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af8ab280-bb42-4961-b1d2-00c2577c1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "data_backup = data.iloc[3524:]\n",
    "data = data.iloc[:3524]\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition_high = seasonal_decompose(data['high'], model='additive', period=30)\n",
    "trend_high = decomposition_high.trend\n",
    "\n",
    "decomposition_low = seasonal_decompose(data['low'], model='additive', period=30)\n",
    "trend_low = decomposition_low.trend\n",
    "\n",
    "data['trend_high'] = trend_high\n",
    "data['trend_low'] = trend_low\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(1,31):\n",
    "    data[f'trend_h+{i}'] = data['trend_high'].shift(i)\n",
    "for i in range(1,31):\n",
    "    data[f'trend_l+{i}'] = data['trend_low'].shift(i)\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb45e3e7-9840-4c33-83a7-72b870eef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_high = data.iloc[:, [8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37]]\n",
    "target_high = data.iloc[:, [6]]\n",
    "\n",
    "inputs_low = data.iloc[:, [38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67]]\n",
    "target_low = data.iloc[:, [7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0c29ce0-c228-44f4-aa82-682e43cd219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(inputs_high, target_high, test_size=0.2, random_state=1, shuffle=False)\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(inputs_low, target_low, test_size=0.2, random_state=1, shuffle=False)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler_h = MinMaxScaler()\n",
    "scaler_h.fit(X_train_h)\n",
    "scaler_target_h = MinMaxScaler()\n",
    "scaler_target_h.fit(y_train_h)\n",
    "\n",
    "scaler_l = MinMaxScaler()\n",
    "scaler_l.fit(X_train_l)\n",
    "scaler_target_l = MinMaxScaler()\n",
    "scaler_target_l.fit(y_train_l)\n",
    "\n",
    "X_train_h_scaled = scaler_h.transform(X_train_h)\n",
    "X_test_h_scaled = scaler_h.transform(X_test_h)\n",
    "y_train_h_scaled = scaler_target_h.transform(y_train_h)\n",
    "y_test_h_scaled = scaler_target_h.transform(y_test_h)\n",
    "\n",
    "X_train_l_scaled = scaler_l.transform(X_train_l)\n",
    "X_test_l_scaled = scaler_l.transform(X_test_l)\n",
    "y_train_l_scaled = scaler_target_l.transform(y_train_l)\n",
    "y_test_l_scaled = scaler_target_l.transform(y_test_l)\n",
    "\n",
    "random_tree_high_30 = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "random_tree_high_30.fit(X_train_h_scaled, y_train_h_scaled)\n",
    "\n",
    "random_tree_low_30 = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "random_tree_low_30.fit(X_train_l_scaled, y_train_l_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04d552-ec6a-483f-a182-9325b16f00f8",
   "metadata": {},
   "source": [
    "#### data reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "957a7a3b-e300-4aa3-8e2e-3ab493ea7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data_backup.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition_high_backup = seasonal_decompose(data_backup['high'], model='additive', period=30)\n",
    "trend_high_backup = decomposition_high_backup.trend\n",
    "\n",
    "decomposition_low_backup = seasonal_decompose(data_backup['low'], model='additive', period=30)\n",
    "trend_low_backup = decomposition_low_backup.trend\n",
    "\n",
    "data_backup['trend_high'] = trend_high_backup\n",
    "data_backup['trend_low'] = trend_low_backup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(1,31):\n",
    "    data_backup[f'trend_h+{i}'] = data_backup['trend_high'].shift(i)\n",
    "for i in range(1,31):\n",
    "    data_backup[f'trend_l+{i}'] = data_backup['trend_low'].shift(i)\n",
    "data_backup.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a36ba365-edc8-4cbf-b37d-e8fa1f616c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_high_backup = data_backup.iloc[:, [8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37]]\n",
    "inputs_low_backup = data_backup.iloc[:, [38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31545c93-98f7-4ef2-ae46-10a7c80757cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = dict()\n",
    "\n",
    "scaler_input = MinMaxScaler()\n",
    "data_backup_h_scaled = scaler_input.fit_transform(inputs_high_backup)\n",
    "scaler_target = MinMaxScaler()\n",
    "target_backup_c_scaled = scaler_target.fit_transform(data_backup[['trend_high']])\n",
    "\n",
    "y_pred_h_backup = random_tree_high_30.predict(data_backup_h_scaled)\n",
    "y_pred_h_backup = y_pred_h_backup.reshape(-1,1)\n",
    "y_pred_h_backup = scaler_target.inverse_transform(y_pred_h_backup)\n",
    "\n",
    "scaler_input = MinMaxScaler()\n",
    "data_backup_l_scaled = scaler_input.fit_transform(inputs_low_backup)\n",
    "scaler_target = MinMaxScaler()\n",
    "target_backup_c_scaled = scaler_target.fit_transform(data_backup[['trend_low']])\n",
    "\n",
    "y_pred_l_backup = random_tree_low_30.predict(data_backup_l_scaled)\n",
    "y_pred_l_backup = y_pred_l_backup.reshape(-1,1)\n",
    "y_pred_l_backup = scaler_target.inverse_transform(y_pred_l_backup)\n",
    "\n",
    "df_pred = {'pred_h':y_pred_h_backup, 'pred_l':y_pred_l_backup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a36b8308-0245-4669-b278-678ad3c5ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS FOR NASDAQ INDEX:\n",
      "score high: 99.25664050968639\n",
      "score low: 105.15383787328467\n",
      "- R2 METRICS FOR NASDAQ INDEX:\n",
      "r2 high: 0.997961661178575\n",
      "r2 low: 0.9976528829386956\n"
     ]
    }
   ],
   "source": [
    "data_backup.reset_index(inplace=True)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score, mean_absolute_error\n",
    "\n",
    "score_h = mean_absolute_error(data_backup[['high']], df_pred['pred_h'])\n",
    "r2_h = r2_score(data_backup[['high']], df_pred['pred_h'])\n",
    "data_backup['mae_h'] = score_h\n",
    "\n",
    "score_l = mean_absolute_error(data_backup[['low']], df_pred['pred_l'])\n",
    "r2_l = r2_score(data_backup[['low']], df_pred['pred_l'])\n",
    "data_backup['mae_l'] = score_l\n",
    "\n",
    "print(f'METRICS FOR NASDAQ INDEX:')\n",
    "print(f'score high: {score_h}')\n",
    "print(f'score low: {score_l}')\n",
    "\n",
    "print(f'- R2 METRICS FOR NASDAQ INDEX:')\n",
    "print(f'r2 high: {r2_h}')\n",
    "print(f'r2 low: {r2_l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e533f675-358d-48c0-8674-7061f3eade1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_columns = ('date','open','high','low','close','volume','trend_high','trend_low','mae_h','mae_l')\n",
    "\n",
    "data_backup = data_backup[list(remaining_columns)]\n",
    "\n",
    "y_pred_h_series = pd.Series(df_pred['pred_h'].reshape(1,-1)[0])\n",
    "y_pred_l_series = pd.Series(df_pred['pred_l'].reshape(1,-1)[0])\n",
    "data_backup['pred_high'] = y_pred_h_series\n",
    "data_backup['pred_low'] = y_pred_l_series\n",
    "\n",
    "data_backup.to_excel(f'data_w_pred_m5/index_w_pred_m5_30bar.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
