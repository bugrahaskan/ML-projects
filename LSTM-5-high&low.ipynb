{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f0a8b1-083f-4913-8db1-d30e4b1c4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data_sample_three_years.csv')\n",
    "\n",
    "data = data.iloc[:, [0,1,2,3,4]]\n",
    "data.rename(columns={'date': 'open', 'low':'close', 'open':'high', 'high':'low'}, inplace=True)\n",
    "\n",
    "data['date'] = pd.to_datetime(data['timestamp'], unit='s')\n",
    "data = data.iloc[:, [0,5,1,2,3,4]]\n",
    "\n",
    "data_backup = data.loc[ data.timestamp >= 1675814400 ]\n",
    "data = data.loc[ data.timestamp < 1675814400 ]\n",
    "\n",
    "from talib import RSI\n",
    "data['rsi_14'] = RSI(data['close'], timeperiod=14)\n",
    "data['rsi_14'] = data['rsi_14'].shift(1)\n",
    "\n",
    "from talib import MA, SMA, EMA, WMA\n",
    "data['ma_9'] = MA(data['close'], timeperiod=9)\n",
    "data['ma_9'] = data['ma_9'].shift(1)\n",
    "data['sma_9'] = SMA(data['close'], timeperiod=9)\n",
    "data['sma_9'] = data['sma_9'].shift(1)\n",
    "data['wma_9'] = WMA(data['close'], timeperiod=9)\n",
    "data['wma_9'] = data['wma_9'].shift(1)\n",
    "\n",
    "from talib import MACD\n",
    "data['macd'], data['signal'], data['hist'] = MACD(data['close'])\n",
    "data['macd'] = data['macd'].shift(1)\n",
    "data['signal'] = data['signal'].shift(1)\n",
    "data['hist'] = data['hist'].shift(1)\n",
    "\n",
    "from talib import ADX\n",
    "data['adx'] = ADX(data['high'], data['low'], data['close'])\n",
    "data['adx'] = data['adx'].shift(1)\n",
    "\n",
    "from talib import ATR\n",
    "data['atr'] = ATR(high=data['high'], low=data['low'], close=data['close'], timeperiod=14)\n",
    "data['atr'] = data['atr'].shift(1)\n",
    "\n",
    "from talib import SAR\n",
    "data['sar'] = SAR(high=data['high'], low=data['low'], acceleration=0.02, maximum=0.2)\n",
    "data['sar'] = data['sar'].shift(1)\n",
    "\n",
    "from talib import TEMA\n",
    "data['tema'] = TEMA(data['close'], timeperiod=14)\n",
    "data['tema'] = data['tema'].shift(1)\n",
    "\n",
    "from talib import ROC\n",
    "data['roc'] = ROC(data['close'], timeperiod=14)\n",
    "data['roc'] = data['roc'].shift(1)\n",
    "\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396ed84f-0592-42cb-934f-f1d4bb83e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_h = ('high', 'ma_9', 'sma_9', 'macd', 'signal', 'hist', 'adx', 'atr', 'sar', 'tema', 'roc')\n",
    "best_features_l = ('low', 'ma_9', 'sma_9', 'macd', 'signal', 'hist', 'adx', 'atr', 'sar', 'tema', 'roc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0813ae8a-ece7-4765-acec-5aec3bcc41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_h = data[list(best_features_h)]\n",
    "data_target_h = data[['high']]\n",
    "\n",
    "data_input_l = data[list(best_features_l)]\n",
    "data_target_l = data[['low']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bfe62f-fded-476a-94cc-951d0e5fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_h = MinMaxScaler()\n",
    "data_scaled_h = scaler_h.fit_transform(data_input_h)\n",
    "\n",
    "scaler_target_h = MinMaxScaler()\n",
    "target_scaled_h = scaler_target_h.fit_transform(data_target_h)\n",
    "\n",
    "scaler_l = MinMaxScaler()\n",
    "data_scaled_l = scaler_l.fit_transform(data_input_l)\n",
    "\n",
    "scaler_target_l = MinMaxScaler()\n",
    "target_scaled_l = scaler_target_l.fit_transform(data_target_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e2d39a-6d3a-4a6c-b28b-14930759b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4  # Number of time steps in each sequence\n",
    "\n",
    "num_features_h = data_input_h.shape[1]\n",
    "num_features_l = data_input_l.shape[1]\n",
    "\n",
    "X_features_h = data_scaled_h\n",
    "y_target_h = target_scaled_h\n",
    "X_features_l = data_scaled_l\n",
    "y_target_l = target_scaled_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ae5f16-4508-4d30-a52c-60df86c46245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create input sequences and targets\n",
    "def create_sequences(features, target, seq_length):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X_seq.append(features[i:i+seq_length])  # Input sequence\n",
    "        y_seq.append(target[i+seq_length]) # Target value (next data point)\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cea00c-b387-49e9-b5ac-0d84fa72f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_h, y_seq_h = create_sequences(X_features_h, y_target_h, seq_length)\n",
    "X_seq_l, y_seq_l = create_sequences(X_features_l, y_target_l, seq_length)\n",
    "\n",
    "# Reshape X_seq to fit LSTM model input shape\n",
    "X_seq_h = X_seq_h.reshape(X_seq_h.shape[0], seq_length, num_features_h)\n",
    "X_seq_l = X_seq_l.reshape(X_seq_l.shape[0], seq_length, num_features_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7f700b4-3592-47d6-9f13-3babe74658eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 11:20:46.965677: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-29 11:20:47.043577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 11:20:47.043659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 11:20:47.047145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 11:20:47.063249: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-29 11:20:48.663125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "289/289 [==============================] - 4s 7ms/step - loss: 0.0073 - val_loss: 8.8789e-05\n",
      "Epoch 2/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 2.1234e-04 - val_loss: 6.7505e-05\n",
      "Epoch 3/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1.7494e-04 - val_loss: 5.0072e-05\n",
      "Epoch 4/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 1.5957e-04 - val_loss: 4.7433e-05\n",
      "Epoch 5/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 1.4992e-04 - val_loss: 9.7659e-05\n",
      "Epoch 6/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1.3958e-04 - val_loss: 3.5190e-05\n",
      "Epoch 7/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1.2867e-04 - val_loss: 4.4623e-05\n",
      "Epoch 8/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1.2073e-04 - val_loss: 2.6592e-05\n",
      "Epoch 9/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1.1176e-04 - val_loss: 2.2429e-05\n",
      "Epoch 10/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1.0079e-04 - val_loss: 2.2638e-05\n",
      "Epoch 11/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 9.6128e-05 - val_loss: 2.6093e-05\n",
      "Epoch 12/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 9.1293e-05 - val_loss: 1.9510e-05\n",
      "Epoch 13/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 9.3473e-05 - val_loss: 2.0688e-05\n",
      "Epoch 14/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 8.2552e-05 - val_loss: 1.9532e-05\n",
      "Epoch 15/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 8.2550e-05 - val_loss: 1.9397e-05\n",
      "Epoch 16/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 8.0424e-05 - val_loss: 1.8239e-05\n",
      "Epoch 17/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.9087e-05 - val_loss: 3.3989e-05\n",
      "Epoch 18/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 8.1062e-05 - val_loss: 1.3834e-05\n",
      "Epoch 19/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.8615e-05 - val_loss: 2.8241e-05\n",
      "Epoch 20/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.6851e-05 - val_loss: 2.1562e-05\n",
      "Epoch 21/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.4177e-05 - val_loss: 1.3253e-05\n",
      "Epoch 22/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.9105e-05 - val_loss: 1.3734e-05\n",
      "Epoch 23/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.5939e-05 - val_loss: 1.3091e-05\n",
      "Epoch 24/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.5214e-05 - val_loss: 1.4084e-05\n",
      "Epoch 25/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.6765e-05 - val_loss: 3.3148e-05\n",
      "Epoch 26/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.7381e-05 - val_loss: 1.3256e-05\n",
      "Epoch 27/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.0691e-05 - val_loss: 2.0268e-05\n",
      "Epoch 28/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.7307e-05 - val_loss: 1.2121e-05\n",
      "Epoch 29/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2872e-05 - val_loss: 1.4072e-05\n",
      "Epoch 30/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.9704e-05 - val_loss: 1.1868e-05\n",
      "Epoch 31/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.7352e-05 - val_loss: 3.3819e-05\n",
      "Epoch 32/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.0535e-05 - val_loss: 1.2231e-05\n",
      "Epoch 33/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 6.3122e-05 - val_loss: 1.2528e-05\n",
      "Epoch 34/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.7476e-05 - val_loss: 1.4784e-05\n",
      "Epoch 35/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.5522e-05 - val_loss: 1.5647e-05\n",
      "Epoch 36/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.3613e-05 - val_loss: 1.2933e-05\n",
      "Epoch 37/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.3908e-05 - val_loss: 1.4917e-05\n",
      "Epoch 38/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.5000e-05 - val_loss: 1.1339e-05\n",
      "Epoch 39/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.0363e-05 - val_loss: 4.8417e-05\n",
      "Epoch 40/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.0942e-05 - val_loss: 1.4724e-05\n",
      "Epoch 41/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.8934e-05 - val_loss: 1.1932e-05\n",
      "Epoch 42/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.8572e-05 - val_loss: 1.1083e-05\n",
      "Epoch 43/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 5.9489e-05 - val_loss: 9.7886e-06\n",
      "Epoch 44/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 5.4826e-05 - val_loss: 1.1341e-05\n",
      "Epoch 45/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 5.5617e-05 - val_loss: 9.8078e-06\n",
      "Epoch 46/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 5.2318e-05 - val_loss: 4.5059e-05\n",
      "Epoch 47/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 5.4104e-05 - val_loss: 1.8763e-05\n",
      "Epoch 48/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.1783e-05 - val_loss: 9.1961e-06\n",
      "Epoch 49/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 5.4775e-05 - val_loss: 3.8479e-05\n",
      "Epoch 50/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 5.3696e-05 - val_loss: 2.2399e-05\n",
      "Epoch 51/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 5.1984e-05 - val_loss: 1.0677e-05\n",
      "Epoch 52/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.8660e-05 - val_loss: 1.1750e-05\n",
      "Epoch 53/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.0819e-05 - val_loss: 1.4260e-05\n",
      "Epoch 54/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.0017e-05 - val_loss: 9.1823e-06\n",
      "Epoch 55/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.7851e-05 - val_loss: 1.2563e-05\n",
      "Epoch 56/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.8135e-05 - val_loss: 1.7563e-05\n",
      "Epoch 57/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.0298e-05 - val_loss: 8.7712e-06\n",
      "Epoch 58/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.8150e-05 - val_loss: 2.6183e-05\n",
      "Epoch 59/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.6648e-05 - val_loss: 1.0688e-05\n",
      "Epoch 60/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.5357e-05 - val_loss: 8.9128e-06\n",
      "Epoch 61/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.8991e-05 - val_loss: 3.4940e-05\n",
      "Epoch 62/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.7072e-05 - val_loss: 1.4917e-05\n",
      "Epoch 63/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 5.0195e-05 - val_loss: 1.0882e-05\n",
      "Epoch 64/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.5603e-05 - val_loss: 9.2928e-06\n",
      "Epoch 65/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.7565e-05 - val_loss: 2.3828e-05\n",
      "Epoch 66/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.5790e-05 - val_loss: 8.5985e-06\n",
      "Epoch 67/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 4.4808e-05 - val_loss: 8.7313e-06\n",
      "Epoch 68/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.7795e-05 - val_loss: 8.3735e-06\n",
      "Epoch 69/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.7186e-05 - val_loss: 1.3092e-05\n",
      "Epoch 70/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 4.5793e-05 - val_loss: 8.4727e-06\n",
      "Epoch 71/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.6249e-05 - val_loss: 9.1224e-06\n",
      "Epoch 72/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.7421e-05 - val_loss: 8.7907e-06\n",
      "Epoch 73/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 4.7190e-05 - val_loss: 1.2280e-05\n",
      "Epoch 74/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.7419e-05 - val_loss: 8.5279e-06\n",
      "Epoch 75/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 4.3692e-05 - val_loss: 1.2766e-05\n",
      "Epoch 76/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 4.5123e-05 - val_loss: 3.7192e-05\n",
      "Epoch 77/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 4.6212e-05 - val_loss: 8.4368e-06\n",
      "Epoch 78/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 4.7060e-05 - val_loss: 1.2263e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f65183d9c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build LSTM model\n",
    "model_h = Sequential([\n",
    "    LSTM(25, input_shape=(seq_length, num_features_h)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_h.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Training the model with early stopping\n",
    "model_h.fit(X_seq_h, y_seq_h, epochs=100, batch_size=45, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bff28e9-0393-4f4a-8b38-98e8f7d6f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "289/289 [==============================] - 4s 7ms/step - loss: 0.0092 - val_loss: 1.6820e-04\n",
      "Epoch 2/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 2.2333e-04 - val_loss: 4.6294e-05\n",
      "Epoch 3/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 2.0540e-04 - val_loss: 4.4793e-05\n",
      "Epoch 4/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 1.9382e-04 - val_loss: 4.0514e-05\n",
      "Epoch 5/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 1.7964e-04 - val_loss: 3.4264e-05\n",
      "Epoch 6/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.6405e-04 - val_loss: 2.7477e-05\n",
      "Epoch 7/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.5320e-04 - val_loss: 2.9457e-05\n",
      "Epoch 8/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.4384e-04 - val_loss: 2.0183e-05\n",
      "Epoch 9/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.3676e-04 - val_loss: 2.4559e-05\n",
      "Epoch 10/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.3296e-04 - val_loss: 1.6488e-05\n",
      "Epoch 11/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.2634e-04 - val_loss: 3.3473e-05\n",
      "Epoch 12/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.2724e-04 - val_loss: 6.9220e-05\n",
      "Epoch 13/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.2835e-04 - val_loss: 1.6282e-05\n",
      "Epoch 14/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 1.1615e-04 - val_loss: 1.9692e-05\n",
      "Epoch 15/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 1.1578e-04 - val_loss: 2.3596e-05\n",
      "Epoch 16/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 1.1647e-04 - val_loss: 1.3995e-05\n",
      "Epoch 17/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.0591e-04 - val_loss: 2.8094e-05\n",
      "Epoch 18/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 1.1081e-04 - val_loss: 1.4433e-05\n",
      "Epoch 19/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 1.1013e-04 - val_loss: 1.2924e-04\n",
      "Epoch 20/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 1.0697e-04 - val_loss: 1.8736e-05\n",
      "Epoch 21/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 1.0390e-04 - val_loss: 1.7407e-05\n",
      "Epoch 22/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 1.0267e-04 - val_loss: 2.3403e-05\n",
      "Epoch 23/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 9.8970e-05 - val_loss: 1.8357e-05\n",
      "Epoch 24/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 9.6956e-05 - val_loss: 1.7218e-05\n",
      "Epoch 25/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 1.0458e-04 - val_loss: 1.3100e-05\n",
      "Epoch 26/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 9.6471e-05 - val_loss: 1.1957e-05\n",
      "Epoch 27/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 9.0840e-05 - val_loss: 1.7912e-05\n",
      "Epoch 28/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 9.3840e-05 - val_loss: 1.4066e-05\n",
      "Epoch 29/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 9.1418e-05 - val_loss: 1.2437e-05\n",
      "Epoch 30/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 8.8823e-05 - val_loss: 1.1230e-05\n",
      "Epoch 31/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 9.4294e-05 - val_loss: 1.1119e-05\n",
      "Epoch 32/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 8.8795e-05 - val_loss: 1.3684e-05\n",
      "Epoch 33/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 8.5374e-05 - val_loss: 1.3096e-05\n",
      "Epoch 34/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 8.2820e-05 - val_loss: 1.0280e-05\n",
      "Epoch 35/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 8.2339e-05 - val_loss: 2.5950e-05\n",
      "Epoch 36/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 8.5170e-05 - val_loss: 2.8295e-05\n",
      "Epoch 37/100\n",
      "289/289 [==============================] - 2s 7ms/step - loss: 7.9944e-05 - val_loss: 1.2714e-05\n",
      "Epoch 38/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.8010e-05 - val_loss: 9.7754e-06\n",
      "Epoch 39/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.8037e-05 - val_loss: 2.0601e-05\n",
      "Epoch 40/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.6971e-05 - val_loss: 1.0282e-05\n",
      "Epoch 41/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 8.0021e-05 - val_loss: 9.4440e-06\n",
      "Epoch 42/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.7411e-05 - val_loss: 1.8193e-05\n",
      "Epoch 43/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.6127e-05 - val_loss: 9.5321e-06\n",
      "Epoch 44/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.5954e-05 - val_loss: 3.6886e-05\n",
      "Epoch 45/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.4953e-05 - val_loss: 2.4503e-05\n",
      "Epoch 46/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.5433e-05 - val_loss: 1.0068e-05\n",
      "Epoch 47/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.4468e-05 - val_loss: 1.2971e-05\n",
      "Epoch 48/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.4944e-05 - val_loss: 1.0241e-05\n",
      "Epoch 49/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.6317e-05 - val_loss: 8.7393e-06\n",
      "Epoch 50/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 7.3488e-05 - val_loss: 1.0026e-05\n",
      "Epoch 51/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.3732e-05 - val_loss: 1.3148e-05\n",
      "Epoch 52/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.2469e-05 - val_loss: 8.5104e-06\n",
      "Epoch 53/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2830e-05 - val_loss: 1.2192e-05\n",
      "Epoch 54/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2507e-05 - val_loss: 1.8298e-05\n",
      "Epoch 55/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2718e-05 - val_loss: 1.1036e-05\n",
      "Epoch 56/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.4931e-05 - val_loss: 4.2152e-05\n",
      "Epoch 57/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.5121e-05 - val_loss: 1.1251e-05\n",
      "Epoch 58/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.0796e-05 - val_loss: 8.6966e-06\n",
      "Epoch 59/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.1169e-05 - val_loss: 9.3777e-06\n",
      "Epoch 60/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.4124e-05 - val_loss: 1.9400e-05\n",
      "Epoch 61/100\n",
      "289/289 [==============================] - 2s 6ms/step - loss: 7.1443e-05 - val_loss: 8.2100e-06\n",
      "Epoch 62/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2203e-05 - val_loss: 4.3107e-05\n",
      "Epoch 63/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2651e-05 - val_loss: 1.3361e-05\n",
      "Epoch 64/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.1226e-05 - val_loss: 1.7397e-05\n",
      "Epoch 65/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 6.9696e-05 - val_loss: 1.0622e-05\n",
      "Epoch 66/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.2248e-05 - val_loss: 1.0781e-05\n",
      "Epoch 67/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.1653e-05 - val_loss: 1.6886e-05\n",
      "Epoch 68/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.0556e-05 - val_loss: 1.1424e-05\n",
      "Epoch 69/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 7.0512e-05 - val_loss: 1.9749e-05\n",
      "Epoch 70/100\n",
      "289/289 [==============================] - 2s 5ms/step - loss: 6.9527e-05 - val_loss: 3.3367e-05\n",
      "Epoch 71/100\n",
      "289/289 [==============================] - 1s 5ms/step - loss: 7.1992e-05 - val_loss: 1.8692e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f650a765000>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build LSTM model\n",
    "model_l = Sequential([\n",
    "    LSTM(25, input_shape=(seq_length, num_features_l)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_l.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Training the model with early stopping\n",
    "model_l.fit(X_seq_l, y_seq_l, epochs=100, batch_size=45, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c395978-6b20-4f28-846c-216043fc1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from talib import RSI\n",
    "data_backup['rsi_14'] = RSI(data_backup['close'], timeperiod=14)\n",
    "data_backup['rsi_14'] = data_backup['rsi_14'].shift(1)\n",
    "\n",
    "from talib import MA, SMA, EMA, WMA\n",
    "data_backup['ma_9'] = MA(data_backup['close'], timeperiod=9)\n",
    "data_backup['ma_9'] = data_backup['ma_9'].shift(1)\n",
    "data_backup['sma_9'] = SMA(data_backup['close'], timeperiod=9)\n",
    "data_backup['sma_9'] = data_backup['sma_9'].shift(1)\n",
    "data_backup['wma_9'] = WMA(data_backup['close'], timeperiod=9)\n",
    "data_backup['wma_9'] = data_backup['wma_9'].shift(1)\n",
    "\n",
    "from talib import MACD\n",
    "data_backup['macd'], data_backup['signal'], data_backup['hist'] = MACD(data_backup['close'])\n",
    "data_backup['macd'] = data_backup['macd'].shift(1)\n",
    "data_backup['signal'] = data_backup['signal'].shift(1)\n",
    "data_backup['hist'] = data_backup['hist'].shift(1)\n",
    "\n",
    "from talib import ADX\n",
    "data_backup['adx'] = ADX(data_backup['high'], data_backup['low'], data_backup['close'])\n",
    "data_backup['adx'] = data_backup['adx'].shift(1)\n",
    "\n",
    "from talib import ATR\n",
    "data_backup['atr'] = ATR(high=data_backup['high'], low=data_backup['low'], close=data_backup['close'], timeperiod=14)\n",
    "data_backup['atr'] = data_backup['atr'].shift(1)\n",
    "\n",
    "from talib import SAR\n",
    "data_backup['sar'] = SAR(high=data_backup['high'], low=data_backup['low'], acceleration=0.02, maximum=0.2)\n",
    "data_backup['sar'] = data_backup['sar'].shift(1)\n",
    "\n",
    "from talib import TEMA\n",
    "data_backup['tema'] = TEMA(data_backup['close'], timeperiod=14)\n",
    "data_backup['tema'] = data_backup['tema'].shift(1)\n",
    "\n",
    "from talib import ROC\n",
    "data_backup['roc'] = ROC(data_backup['close'], timeperiod=14)\n",
    "data_backup['roc'] = data_backup['roc'].shift(1)\n",
    "\n",
    "data_backup.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d910ce2b-00a8-415e-83e2-5fdfb7fa0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup_h_scaled = scaler_h.transform(data_backup[list(best_features_h)])\n",
    "target_backup_h_scaled = scaler_target_h.transform(data_backup[['high']])\n",
    "X_b_h, y_b_h = create_sequences(data_backup_h_scaled, target_backup_h_scaled, seq_length)\n",
    "X_b_h = X_b_h.reshape(X_b_h.shape[0], seq_length, num_features_h)\n",
    "\n",
    "data_backup_l_scaled = scaler_l.transform(data_backup[list(best_features_l)])\n",
    "target_backup_l_scaled = scaler_target_l.transform(data_backup[['low']])\n",
    "X_b_l, y_b_l = create_sequences(data_backup_l_scaled, target_backup_l_scaled, seq_length)\n",
    "X_b_l = X_b_l.reshape(X_b_l.shape[0], seq_length, num_features_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b10846b-b415-466f-ad60-ff33de5f54b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 1s 2ms/step\n",
      "252/252 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_h = model_h.predict(X_b_h)\n",
    "y_pred_h = scaler_target_h.inverse_transform(y_pred_h)\n",
    "\n",
    "y_pred_l = model_l.predict(X_b_l)\n",
    "y_pred_l = scaler_target_l.inverse_transform(y_pred_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b8bd5d7-576f-414f-a035-3de1c8d909d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f5f78f-31bf-4451-bc1b-ce33dcd27f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce061c6b-e333-4473-9f91-be2438fceae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score high: 153.24242150106053\n",
      "score low: 124.35153152658478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "score_h = mean_squared_error(data_backup[['high']].iloc[seq_length:], y_pred_h)\n",
    "score_l = mean_squared_error(data_backup[['low']].iloc[seq_length:], y_pred_l)\n",
    "\n",
    "print(f'score high: {score_h}')\n",
    "print(f'score low: {score_l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34a3c7c0-8c5a-4d93-8e1d-7b5c62df0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_h_series = pd.Series(y_pred_h.reshape(1,-1)[0])\n",
    "y_pred_l_series = pd.Series(y_pred_l.reshape(1,-1)[0])\n",
    "data_backup['pred_highs'] = y_pred_h_series\n",
    "data_backup['pred_lows'] = y_pred_l_series\n",
    "data_backup.to_excel('lstm-multiple-factor-higs&lows.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffac9ba-ac1f-4853-bf38-60a7ca988791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc53cbf-6389-4835-9766-fe6273053784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efd31c-f328-49f1-904b-c59329bb999e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90450f-2f40-4773-833b-1e37d203385e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
