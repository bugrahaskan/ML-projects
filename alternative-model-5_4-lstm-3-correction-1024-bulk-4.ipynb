{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78094f3d-9814-4780-8886-b692af3ed902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and copied.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "data_backup = data.iloc[3524:]\n",
    "\n",
    "data = data.iloc[:3524]\n",
    "data_copy = data.copy()\n",
    "\n",
    "print('Data imported and copied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db92254e-9cc3-4333-a29d-6428c074cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Creating sequences\n",
    "def create_dataset(dataset, time_step=1, output_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-output_step):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        b = dataset[(i+time_step):(i+time_step)+output_step, 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(b)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "period = 60\n",
    "trend_period = 14\n",
    "num_features = 1\n",
    "input_period = 46\n",
    "output_step = 7\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9d8f90-ded5-447b-a745-1cd01a9f6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 07:35:10.842461: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-23 07:35:10.920260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-23 07:35:10.920325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-23 07:35:10.923819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-23 07:35:10.939251: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-23 07:35:12.900132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing the Model...', flush=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=units, input_shape=(input_period, num_features)),\n",
    "    #LSTM(units=units, return_sequences=True, input_shape=(input_period, num_features)),\n",
    "    #Dropout(0.2),\n",
    "    #LSTM(units=units, return_sequences=False),\n",
    "    #Dropout(0.2),\n",
    "    Dense(output_step)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_2/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print('Model has been initialized.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef33120-6f01-4e2a-8087-9fcc20188cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup: Weigths for the best epoch has been loaded.\n"
     ]
    }
   ],
   "source": [
    "best_epoch_backup = 94\n",
    "\n",
    "# Load the weights of the model at the chosen epoch\n",
    "model.load_weights(f'model_weights_2/model_weights_epoch_{best_epoch_backup:02d}.h5')\n",
    "print('Backup: Weigths for the best epoch has been loaded.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578ce704-4cca-4913-b914-73bb20e4ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the bulk data...\n",
      "Bulk data has been imported.\n"
     ]
    }
   ],
   "source": [
    "print('Importing the bulk data...', flush=True)\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "directory = 'data-china-from2018-test'\n",
    "\n",
    "df_data = dict()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    symbol_pattern = re.match(r'([^_]+)_', file)\n",
    "    symbol = symbol_pattern.group(1)\n",
    "    df_data[symbol] = pd.read_csv(os.path.join(directory, file))\n",
    "    df_data[symbol].rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "print('Bulk data has been imported.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4996f0-bb24-44d6-8eee-17ff52d90c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the data sets...\n",
      "Data sets has been created.\n"
     ]
    }
   ],
   "source": [
    "print('Creating the data sets...', flush=True)\n",
    "\n",
    "dict_X_test, dict_y_test = dict(), dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    X_test, y_test = create_dataset(df_data[symbol][['close']].to_numpy(), time_step=period, output_step=output_step)\n",
    "\n",
    "    dict_X_test[symbol] = X_test\n",
    "    dict_y_test[symbol] = y_test\n",
    "\n",
    "print('Data sets has been created.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba385a5-f88d-44aa-a44b-42b844940dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing data...\n",
      "Decomposition has been applied.\n"
     ]
    }
   ],
   "source": [
    "print('Decomposing data...', flush=True)\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "dict_decompositions = dict()\n",
    "dict_trends = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    decompositions_test = np.array([seasonal_decompose(dict_X_test[symbol][i], model='additive', period=14) for i in range(dict_X_test[symbol].shape[0])])\n",
    "    trends_test = np.array([decompositions_test[i].trend for i in range(decompositions_test.shape[0])])\n",
    "\n",
    "    dict_decompositions[symbol] = decompositions_test\n",
    "    dict_trends[symbol] = trends_test\n",
    "\n",
    "print('Decomposition has been applied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e5855b-8b63-44ca-8bbc-d9826c6f5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n",
      "Data scaled.\n"
     ]
    }
   ],
   "source": [
    "print('Scaling data...', flush=True)\n",
    "\n",
    "dict_trends_dropna = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    trends_test_dropna = list()\n",
    "    \n",
    "    for trend in dict_trends[symbol]:\n",
    "        trends_test_dropna.append(trend[~np.isnan(trend)])\n",
    "\n",
    "    trends_test_dropna = np.array(trends_test_dropna)\n",
    "\n",
    "    dict_trends_dropna[symbol] = trends_test_dropna\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dict_scalers_trend = dict()\n",
    "dict_scalers_target = dict()\n",
    "dict_trends_scaled = dict()\n",
    "dict_targets_scaled = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    scaler_trend = list(MinMaxScaler() for i in range(dict_trends_dropna[symbol].shape[0]))\n",
    "    trends_test_scaled = list()\n",
    "\n",
    "    # we use target values only for comparison issue here\n",
    "    scaler_target = list(MinMaxScaler() for i in range(dict_y_test[symbol].shape[0]))\n",
    "    target_test_scaled = list()\n",
    "\n",
    "    for i in range(dict_trends_dropna[symbol].shape[0]):\n",
    "        trends_test_scaled.append(scaler_trend[i].fit_transform(dict_trends_dropna[symbol][i].reshape(-1,1)))\n",
    "\n",
    "    for j in range(dict_y_test[symbol].shape[0]):\n",
    "        target_test_scaled.append(scaler_target[j].fit_transform(dict_y_test[symbol][j].reshape(-1,1)))\n",
    "\n",
    "    trends_test_scaled = np.array(trends_test_scaled)\n",
    "    target_test_scaled = np.array(target_test_scaled)\n",
    "\n",
    "    dict_scalers_trend[symbol] = scaler_trend\n",
    "    dict_scalers_target[symbol] = scaler_target\n",
    "    dict_trends_scaled[symbol] = trends_test_scaled\n",
    "    dict_targets_scaled[symbol] = target_test_scaled\n",
    "\n",
    "print('Data scaled.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4210cec3-2620-47fd-8886-9162b9856204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [1:20:40, 1613.53s/it]\n"
     ]
    }
   ],
   "source": [
    "print('Predicting...', flush=True)\n",
    "\n",
    "dict_predictions = dict()\n",
    "\n",
    "from tqdm import tqdm\n",
    "for nb, symbol in tqdm(enumerate(df_data)):\n",
    "    trend_predictions = list()\n",
    "\n",
    "    for j in range(dict_trends_scaled[symbol].shape[0]):\n",
    "\n",
    "        res = list()\n",
    "        trend = dict_trends_scaled[symbol][j]\n",
    "        \n",
    "        for i in range(output_step):\n",
    "            res.append(\n",
    "                model.predict(trend.reshape(trend.shape[1], input_period, num_features), verbose=0)\n",
    "            )\n",
    "            \n",
    "            trend = np.append(trend[1:], np.array(res[-1][0][0])).reshape(input_period, num_features)\n",
    "\n",
    "        trend_predictions.append(\n",
    "            dict_scalers_trend[symbol][j].inverse_transform(\n",
    "                trend[-7:]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    dict_predictions[symbol] = np.array(trend_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "221ce7d0-b117-4906-bcd6-22944779b627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to data...\n",
      "Noise to data has been added.\n"
     ]
    }
   ],
   "source": [
    "print('Adding noise to data...', flush=True)\n",
    "\n",
    "dict_std_dev_noise = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    noise_high, _ = create_dataset(df_data[symbol][['high']].to_numpy(), time_step=period, output_step=output_step)\n",
    "    noise_low, _ = create_dataset(df_data[symbol][['low']].to_numpy(), time_step=period, output_step=output_step)\n",
    "\n",
    "    std_dev_high = np.array([np.std(val) for val in noise_high])\n",
    "    std_dev_low = np.array([np.std(val) for val in noise_low])\n",
    "    \n",
    "    std_dev_noise = np.array([np.maximum(val_high, val_low) for val_high, val_low in zip(std_dev_high, std_dev_low)])\n",
    "\n",
    "    dict_std_dev_noise[symbol] = std_dev_noise\n",
    "\n",
    "print('Noise to data has been added.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5457927c-f158-49e2-b797-02beb8d42668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing observations for data...\n",
      "Observations have been constructed.\n"
     ]
    }
   ],
   "source": [
    "print('Constructing observations for data...', flush=True)\n",
    "\n",
    "dict_observations = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    observation = pd.Series([\n",
    "        dict_predictions[symbol][i] for i in range(dict_predictions[symbol].shape[0])\n",
    "    ], index=df_data[symbol].index[output_step:-period])\n",
    "\n",
    "    dict_observations[symbol] = observation\n",
    "\n",
    "print('Observations have been constructed.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ae4345-c0d9-41f8-98e5-4246e4a33887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 done for 000063.SZ\n",
      "2 done for 000001.SS\n",
      "3 done for 000002.SZ\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data_output = df_data.copy()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for symbol in df_data:\n",
    "    count += 1\n",
    "\n",
    "    _obs = [dict_observations[symbol][k+7] for k in range(dict_observations[symbol].shape[0])]\n",
    "    _res = []\n",
    "    for obs in _obs:\n",
    "        _res.append(obs.reshape(-1,output_step)[0])\n",
    "    \n",
    "    observation_df = pd.DataFrame([r for r in _res], columns=[f'pred_{i}' for i in range(output_step)], index=df_data[symbol].index[7:-60])\n",
    "    data_output[symbol] = pd.concat([data_output[symbol], observation_df], axis=1, join='inner')\n",
    "\n",
    "    noise_df = pd.DataFrame(dict_std_dev_noise[symbol], columns=['noise'], index=df_data[symbol].index[7:-60])\n",
    "    data_output[symbol] = pd.concat([data_output[symbol], noise_df], axis=1, join='inner')\n",
    "\n",
    "    for i in range(output_step):\n",
    "        data_output[symbol][f'pred_{i}'] = data_output[symbol][f'pred_{i}'].shift(53)\n",
    "\n",
    "    data_output[symbol].to_excel(f'data_w_pred_m5-china-test/{symbol}_w_pred_m5.xlsx')\n",
    "    \n",
    "    print(f'{count} done for {symbol}', flush=True)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Pickle the object and save it to a file\n",
    "with open('data_pred-china-bulk.pickle', 'wb') as f:\n",
    "    pickle.dump(dict_predictions, f)\n",
    "\n",
    "print('Done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53adff-e95c-4104-8a59-2552f09a2328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f936b7-cca6-499a-983f-23ac51bb3226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
