{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337577f1-f86d-4b21-941d-accdab6bfa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and copied.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "data_backup = data.iloc[3524:]\n",
    "\n",
    "data = data.iloc[:3524]\n",
    "data_copy = data.copy()\n",
    "\n",
    "print('Data imported and copied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324e457e-ca72-42a3-a739-d192fbd79e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Creating sequences\n",
    "def create_dataset(dataset, time_step=1, output_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-output_step):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        b = dataset[(i+time_step):(i+time_step)+output_step, 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(b)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "period = 60\n",
    "trend_period = 14\n",
    "rsi_period = 14\n",
    "num_features = 1\n",
    "input_period = 60\n",
    "output_step = 7\n",
    "units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5571abb-b7b1-42fb-8de6-08baf4c76ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 10:32:37.485966: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-06 10:32:38.026909: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-06 10:32:38.027146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-06 10:32:38.120321: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-06 10:32:38.252692: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-06 10:32:41.876271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing the Model...', flush=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Conv1D, AveragePooling1D, Flatten, Reshape, SimpleRNN, GRU, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import Zeros\n",
    "\n",
    "inputs = Input(shape=(input_period, num_features))\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "    Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dense(units=128),\n",
    "    Flatten()\n",
    "    #Dense(units=output_step),\n",
    "    #Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "model_bilstm = Sequential([\n",
    "    Bidirectional(LSTM(units=512, return_sequences=True, activation='tanh', recurrent_activation='sigmoid')),\n",
    "    Dropout(0.2),\n",
    "    Flatten()\n",
    "    #Dense(units=output_step),\n",
    "    #Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "model_bigru = Sequential([\n",
    "    Bidirectional(GRU(units=512, activation='tanh', return_sequences=True)),\n",
    "    Dropout(0.4),\n",
    "    Flatten()\n",
    "])\n",
    "\n",
    "model_multilayer_lstm = Sequential([\n",
    "    LSTM(units=256, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=256, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'),\n",
    "    Flatten()\n",
    "])\n",
    "\n",
    "output_cnn = model_cnn(inputs)\n",
    "output_bilstm = model_bilstm(inputs)\n",
    "output_bigru = model_bigru(inputs)\n",
    "output_multilayer_lstm = model_multilayer_lstm(inputs)\n",
    "\n",
    "concatenated_outputs = concatenate([output_cnn, output_bilstm, output_bigru, output_multilayer_lstm])\n",
    "\n",
    "main_model = Sequential([\n",
    "    Input(shape=(concatenated_outputs.shape[1],)),\n",
    "    Dense(units=output_step, kernel_initializer=Zeros(), use_bias=False),\n",
    "    Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "final_output = main_model(concatenated_outputs)\n",
    "\n",
    "functional_pipeline = Model(inputs=inputs, outputs=final_output)\n",
    "\n",
    "# Compile the pipeline model\n",
    "functional_pipeline.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "print('Model has been initialized.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3c9e9a-a2b2-4d2f-ab5f-cba8c0c53846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 52 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup: Weigths for the best epoch has been loaded.\n"
     ]
    }
   ],
   "source": [
    "best_epoch_backup = 98\n",
    "\n",
    "# Load the weights of the model at the chosen epoch\n",
    "functional_pipeline.load_weights(f'model_weights_6_4_simple-w-savgol/model_weights_epoch_{best_epoch_backup:02d}.weights.h5')\n",
    "print('Backup: Weigths for the best epoch has been loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec0baa1-146b-42ae-881a-f8f73d87c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the bulk data...\n",
      "Bulk data has been imported.\n"
     ]
    }
   ],
   "source": [
    "print('Importing the bulk data...', flush=True)\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "directory = 'data-china-from2018-test'\n",
    "\n",
    "df_data = dict()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    symbol_pattern = re.match(r'([^_]+)_', file)\n",
    "    symbol = symbol_pattern.group(1)\n",
    "    df_data[symbol] = pd.read_csv(os.path.join(directory, file))\n",
    "    df_data[symbol].rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "print('Bulk data has been imported.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b976244-fd9a-433c-98d2-7ce14feb0f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the data sets...\n",
      "Data sets has been created.\n"
     ]
    }
   ],
   "source": [
    "print('Creating the data sets...', flush=True)\n",
    "\n",
    "dict_X_test, dict_y_test = dict(), dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    X_test, y_test = create_dataset(df_data[symbol][['close']].to_numpy(), time_step=period, output_step=output_step)\n",
    "\n",
    "    dict_X_test[symbol] = X_test\n",
    "    dict_y_test[symbol] = y_test\n",
    "\n",
    "print('Data sets has been created.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4702b55d-4d5f-4e25-a204-ca7df254b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing data...\n",
      "Decomposition has been applied.\n"
     ]
    }
   ],
   "source": [
    "print('Decomposing data...', flush=True)\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "window_length = 11  # Window length (must be odd)\n",
    "polyorder = 2      # Polynomial order\n",
    "\n",
    "dict_savgol = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    savgol_test = np.array([ savgol_filter(dict_X_test[symbol][i], window_length, polyorder) for i in range(dict_X_test[symbol].shape[0]) ])\n",
    "\n",
    "    dict_savgol[symbol] = savgol_test\n",
    "\n",
    "print('Decomposition has been applied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26988f67-0aea-41a5-9a7f-942869841733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n",
      "Data scaled.\n"
     ]
    }
   ],
   "source": [
    "print('Scaling data...', flush=True)\n",
    "\n",
    "dict_savgol_dropna = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    savgol_test_dropna = list()\n",
    "    \n",
    "    for _savgol in dict_savgol[symbol]:\n",
    "        savgol_test_dropna.append(_savgol)\n",
    "\n",
    "    savgol_test_dropna = np.array(savgol_test_dropna)\n",
    "\n",
    "    dict_savgol_dropna[symbol] = savgol_test_dropna\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dict_scalers_savgol = dict()\n",
    "dict_scalers_target = dict()\n",
    "dict_savgol_scaled = dict()\n",
    "dict_targets_scaled = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    scaler_savgol = list(MinMaxScaler() for i in range(dict_savgol_dropna[symbol].shape[0]))\n",
    "    savgol_test_scaled = list()\n",
    "\n",
    "    # we use target values only for comparison issue here\n",
    "    scaler_target = list(MinMaxScaler() for i in range(dict_y_test[symbol].shape[0]))\n",
    "    target_test_scaled = list()\n",
    "\n",
    "    for i in range(dict_savgol_dropna[symbol].shape[0]):\n",
    "        savgol_test_scaled.append(scaler_savgol[i].fit_transform(dict_savgol_dropna[symbol][i].reshape(-1,1)))\n",
    "\n",
    "    for j in range(dict_y_test[symbol].shape[0]):\n",
    "        target_test_scaled.append(scaler_target[j].fit_transform(dict_y_test[symbol][j].reshape(-1,1)))\n",
    "\n",
    "    savgol_test_scaled = np.array(savgol_test_scaled)\n",
    "    target_test_scaled = np.array(target_test_scaled)\n",
    "\n",
    "    dict_scalers_savgol[symbol] = scaler_savgol\n",
    "    dict_scalers_target[symbol] = scaler_target\n",
    "    dict_savgol_scaled[symbol] = savgol_test_scaled\n",
    "    dict_targets_scaled[symbol] = target_test_scaled\n",
    "\n",
    "print('Data scaled.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdfca5f-9cb1-44a6-868f-911b2e5748b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing input data...\n",
      "Input data has been prepared.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing input data...', flush=True)\n",
    "\n",
    "dict_x_input = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    x_input = list()\n",
    "\n",
    "    for savgol in dict_savgol_scaled[symbol]:\n",
    "        x_input.append(savgol)\n",
    "\n",
    "    x_input = np.array(x_input)\n",
    "\n",
    "    dict_x_input[symbol] = x_input\n",
    "\n",
    "print('Input data has been prepared.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe368ef-ff93-4c12-aaf1-269902b060fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [06:03, 363.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [14:02, 431.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [19:41, 393.67s/it]\n"
     ]
    }
   ],
   "source": [
    "print('Predicting...', flush=True)\n",
    "\n",
    "dict_predictions = dict()\n",
    "\n",
    "from tqdm import tqdm\n",
    "for nb, symbol in tqdm(enumerate(df_data)):\n",
    "    savgol_predictions = list()\n",
    "\n",
    "    for j in range(dict_x_input[symbol].shape[0]):\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(j+1)\n",
    "            \n",
    "        savgol_predictions.append(\n",
    "            dict_scalers_savgol[symbol][j].inverse_transform(\n",
    "                functional_pipeline.predict(dict_x_input[symbol][j].reshape(savgol_test_scaled[j].shape[1], input_period, num_features), verbose=0)[0].reshape(1,output_step)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    savgol_predictions = np.array(savgol_predictions)\n",
    "\n",
    "    dict_predictions[symbol] = savgol_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "785ac0c5-8629-492b-b0f9-4ee5fbf055ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to data...\n",
      "Noise to data has been added.\n"
     ]
    }
   ],
   "source": [
    "print('Adding noise to data...', flush=True)\n",
    "\n",
    "dict_std_dev_noise = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    noise_high, _ = create_dataset(df_data[symbol][['high']].to_numpy(), time_step=period, output_step=output_step)\n",
    "    noise_low, _ = create_dataset(df_data[symbol][['low']].to_numpy(), time_step=period, output_step=output_step)\n",
    "\n",
    "    std_dev_high = np.array([np.std(val) for val in noise_high])\n",
    "    std_dev_low = np.array([np.std(val) for val in noise_low])\n",
    "    \n",
    "    std_dev_noise = np.array([np.maximum(val_high, val_low) for val_high, val_low in zip(std_dev_high, std_dev_low)])\n",
    "\n",
    "    dict_std_dev_noise[symbol] = std_dev_noise\n",
    "\n",
    "print('Noise to data has been added.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700e7feb-f16a-4b77-ae4c-1f7e36d38af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing observations for data...\n",
      "Observations have been constructed.\n"
     ]
    }
   ],
   "source": [
    "print('Constructing observations for data...', flush=True)\n",
    "\n",
    "dict_observations = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    observation = pd.Series([\n",
    "        dict_predictions[symbol][i] for i in range(dict_predictions[symbol].shape[0])\n",
    "    ], index=df_data[symbol].index[input_period:-output_step])\n",
    "\n",
    "    dict_observations[symbol] = observation\n",
    "\n",
    "print('Observations have been constructed.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73947429-9418-4ad9-92a9-aaaf01f4c869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 done for 000063.SZ\n",
      "2 done for 000001.SS\n",
      "3 done for 000002.SZ\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data_output = df_data.copy()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for symbol in df_data:\n",
    "    count += 1\n",
    "\n",
    "    _obs = [dict_observations[symbol][k+input_period] for k in range(dict_observations[symbol].shape[0])]\n",
    "    _res = []\n",
    "    for obs in _obs:\n",
    "        _res.append(obs.reshape(-1,output_step)[0])\n",
    "    \n",
    "    observation_df = pd.DataFrame([r for r in _res], columns=[f'pred_{i}' for i in range(output_step)], index=df_data[symbol].index[60:-7])\n",
    "    data_output[symbol] = pd.concat([data_output[symbol], observation_df], axis=1, join='inner')\n",
    "\n",
    "    noise_df = pd.DataFrame(dict_std_dev_noise[symbol], columns=['noise'], index=df_data[symbol].index[60:-7])\n",
    "    data_output[symbol] = pd.concat([data_output[symbol], noise_df], axis=1, join='inner')\n",
    "\n",
    "    #for i in range(output_step):\n",
    "    #    data_output[symbol][f'pred_{i}'] = data_output[symbol][f'pred_{i}'].shift(53)\n",
    "\n",
    "    data_output[symbol].to_excel(f'data_w_pred_m6_4-simple-w-savgol-china-test/{symbol}_w_pred_m5.xlsx')\n",
    "    \n",
    "    print(f'{count} done for {symbol}', flush=True)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Pickle the object and save it to a file\n",
    "with open('data_pred-model_6_4-simple-w-savgol-china-test.pickle', 'wb') as f:\n",
    "    pickle.dump(dict_predictions, f)\n",
    "\n",
    "print('Done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0296b-c11d-4970-9077-67483a046280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20595288-ed35-43b1-940c-bc58ec03e9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b62e1f-df6a-4ca7-b06e-2271b5aa5374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28562c19-fb3e-4d9b-b2a4-c0fbabe3a14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba129c5-d87c-4ce5-a02b-4b032e285ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d066c9a-f37d-4c43-8efe-d815253e3ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
