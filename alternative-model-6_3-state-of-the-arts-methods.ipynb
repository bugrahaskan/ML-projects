{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701c3de0-2caa-47a0-b003-de7dbb4b7c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and copied.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "data_backup = data.iloc[3524:]\n",
    "\n",
    "data = data.iloc[:3524]\n",
    "data_copy = data.copy()\n",
    "\n",
    "print('Data imported and copied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f10fd6-eb38-4d8e-8273-cc291d0e292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Creating sequences\n",
    "def create_dataset(dataset, time_step=1, output_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-output_step):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        b = dataset[(i+time_step):(i+time_step)+output_step, 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(b)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "period = 60\n",
    "trend_period = 14\n",
    "rsi_period = 14\n",
    "num_features = 3\n",
    "input_period = 46\n",
    "output_step = 7\n",
    "units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46088f64-d1b4-41f6-ba71-5b4eb357c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(data[['close']].to_numpy(), time_step=period, output_step=output_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b91c775-12cb-4200-998e-a8be3d9af59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = np.array([np.concatenate((X[i], y[i]), axis=0) for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8adc375-912d-4729-bafc-c86324365ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decompositions = np.array([seasonal_decompose(data_input[i], model='additive', period=14) for i in range(data_input.shape[0])])\n",
    "trends = np.array([decompositions[i].trend for i in range(decompositions.shape[0])])\n",
    "seasons = np.array([decompositions[i].seasonal for i in range(decompositions.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fb7279-5ac5-424c-b683-b714a2ccf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from talib import RSI\n",
    "\n",
    "rsi = np.array([ RSI(data_input[i]) for i in range(data_input.shape[0]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33a600d-a762-46a3-821c-b8fa1424cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_dropna = list()\n",
    "seasons_cropped = list()\n",
    "rsi_dropna = list()\n",
    "\n",
    "for trend in trends:\n",
    "    trends_dropna.append(trend[~np.isnan(trend)])\n",
    "\n",
    "for season in seasons:\n",
    "    #seasons_cropped.append(season[int(trend_period/2):-int(trend_period/2)])\n",
    "    seasons_cropped.append(season[trend_period:])\n",
    "\n",
    "for r in rsi:\n",
    "    #rsi_dropna.append(r[-53:])\n",
    "    rsi_dropna.append(r[rsi_period:])\n",
    "\n",
    "trends_dropna = np.array(trends_dropna)\n",
    "seasons_cropped = np.array(seasons_cropped)\n",
    "rsi_dropna = np.array(rsi_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e45886-adef-4cc0-9ec8-57d0edc62ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = list(StandardScaler() for i in range(trends_dropna.shape[0]))\n",
    "trends_scaled = list()\n",
    "\n",
    "scaler_seasonal = list(StandardScaler() for i in range(seasons_cropped.shape[0]))\n",
    "seasons_scaled = list()\n",
    "\n",
    "scaler_rsi = list(MinMaxScaler() for i in range(rsi_dropna.shape[0]))\n",
    "rsi_scaled = list()\n",
    "\n",
    "for i in range(trends_dropna.shape[0]):\n",
    "    trends_scaled.append(scaler[i].fit_transform(trends_dropna[i].reshape(-1,1)))\n",
    "\n",
    "for i in range(seasons_cropped.shape[0]):\n",
    "    seasons_scaled.append(scaler_seasonal[i].fit_transform(seasons_cropped[i].reshape(-1,1)))\n",
    "\n",
    "for i in range(rsi_dropna.shape[0]):\n",
    "    rsi_scaled.append(scaler_rsi[i].fit_transform(rsi_dropna[i].reshape(-1,1)))\n",
    "\n",
    "trends_scaled = np.array(trends_scaled)\n",
    "seasons_scaled = np.array(seasons_scaled)\n",
    "rsi_scaled = np.array(rsi_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c21df1-3b1a-45b2-82b0-0c365a1d3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = list()\n",
    "y_input = list()\n",
    "\n",
    "for trend, season, rsi in zip(trends_scaled, seasons_scaled, rsi_scaled):\n",
    "    X_input.append(np.hstack((trend[:trend.shape[0]-7], season[:season.shape[0]-7], rsi[:rsi.shape[0]-7])))\n",
    "    y_input.append(trend[trend.shape[0]-7:])\n",
    "\n",
    "X_input = np.array(X_input)\n",
    "y_input = np.array(y_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3e91a-ccbd-4dbd-aee8-99ef735a4c9a",
   "metadata": {},
   "source": [
    "## BiLSTM channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "222c7211-c4fa-449c-aca8-ef284669021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 18:07:53.503061: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 18:07:53.571269: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-27 18:07:53.571336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-27 18:07:53.574815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-27 18:07:53.598295: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 18:07:55.188894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Build LSTM model\n",
    "model_channel_1 = Sequential([\n",
    "    Bidirectional(LSTM(units=192*2, activation='tanh', recurrent_activation='sigmoid'), input_shape=(input_period, num_features)),\n",
    "    Dropout(0.2),\n",
    "    Dense(output_step)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_1/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29281917-da4f-403c-8cd1-a331486ec315",
   "metadata": {},
   "source": [
    "## CNN channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b784313a-c281-4bae-97db-1c1d6613b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, Flatten, Dense, Dropout, Reshape\n",
    "\n",
    "model_channel_2 = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=1),\n",
    "    AveragePooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(units=192),\n",
    "    Dense(units=output_step),\n",
    "    Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_2/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4d4b9-577e-4b3c-9651-be9a4ee3ccb7",
   "metadata": {},
   "source": [
    "## RNN channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c799f0-a46e-47ef-94b9-abded860b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Flatten\n",
    "from tensorflow.keras.initializers import Zeros\n",
    "\n",
    "model_channel_3 = Sequential([\n",
    "    SimpleRNN(units=128, activation='tanh', input_shape=(input_period, num_features)),\n",
    "    Dropout(0.1),\n",
    "    Flatten(),\n",
    "    Dense(units=output_step, kernel_initializer=Zeros(), use_bias=False),\n",
    "    Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_3/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fa13f-1751-4ecc-9291-e23848b05ebd",
   "metadata": {},
   "source": [
    "## LSTM channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ecdc671-f5e2-417d-82e5-ab2624b5333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import Zeros\n",
    "\n",
    "# Build LSTM model\n",
    "model_channel_4 = Sequential([\n",
    "    LSTM(units=480, input_shape=(input_period, num_features), activation='tanh', recurrent_activation='sigmoid'),\n",
    "    Dense(units=output_step, kernel_initializer=Zeros(), use_bias=False),\n",
    "    Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_4.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_4/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239db91f-6398-404a-948c-11f9fa2f7430",
   "metadata": {},
   "source": [
    "## Stacked LSTM channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf1e2cfa-6d96-4e73-bb27-e5bc55833eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import Zeros\n",
    "\n",
    "# Build LSTM model\n",
    "model_channel_5 = Sequential([\n",
    "    LSTM(units=512, return_sequences=True, input_shape=(input_period, num_features), activation='tanh', recurrent_activation='sigmoid'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=512, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'),\n",
    "    Flatten(),\n",
    "    Dense(units=output_step, kernel_initializer=Zeros(), use_bias=False),\n",
    "    Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_5.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_5/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f2e79-5289-489b-86a1-0e9c6f3ae40b",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fceffb46-b8e6-4018-9749-899a7f550a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model_channel_6 = Sequential([\n",
    "    Dense(units=512, activation='tanh', input_shape=(input_period, num_features)),\n",
    "    Dropout(0.1),\n",
    "    Dense(output_step, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_6.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_6/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef4385-f076-44a1-80fc-e1b206fa0ae8",
   "metadata": {},
   "source": [
    "## GRU channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae26e14-e1bd-452d-a4d0-f910d975e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Flatten\n",
    "\n",
    "model_channel_7 = Sequential([\n",
    "    GRU(units=192, activation='tanh', return_sequences=True, input_shape=(input_period, num_features)),\n",
    "    Dropout(0.4),\n",
    "    Flatten(),\n",
    "    Dense(units=output_step, kernel_initializer=Zeros(), use_bias=False),\n",
    "    Reshape((output_step,1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_channel_7.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_6_3_channel_7/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064dfd92-f177-4c05-9a6e-e286e709d393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554765d9-939d-4a90-b8bc-43b843ee15b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a291141-5936-4e0a-8bed-c1f2e8c52aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e90399-202b-4847-9bfa-77356c55f3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
