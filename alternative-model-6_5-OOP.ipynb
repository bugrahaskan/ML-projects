{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1220b8ba-940f-48e8-a3f4-033c71700e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class model6_5:\n",
    "\n",
    "    def __init__(self, *args, ohlc='close', csv='data/^NDX_raw_data.csv'):\n",
    "        self.ohlc = ohlc\n",
    "\n",
    "        self.period = 60\n",
    "        self.trend_period = 14\n",
    "        self.rsi_period = 14\n",
    "        self.num_features = 1\n",
    "        self.input_period = 60\n",
    "        self.output_step = 7\n",
    "        self.units = 512\n",
    "\n",
    "        self.components = {'savgol', 'seasons'}\n",
    "        \n",
    "        if not args:\n",
    "            print('Enter model components')\n",
    "            exit(0)\n",
    "        else:\n",
    "            self.model_dict = dict()\n",
    "            for arg in args:\n",
    "                assert arg.lower() in self.components\n",
    "                self.model_dict[arg] = self.create_model(component=arg)\n",
    "\n",
    "    # Logistic transformation\n",
    "    @staticmethod\n",
    "    def logistic_transform(r2):\n",
    "        return round(1 / (1 + np.exp(-r2)),2)\n",
    "    \n",
    "    def report_score(self, outputs):\n",
    "        from sklearn.metrics import r2_score\n",
    "        \n",
    "        pred, y = outputs\n",
    "        \n",
    "        try:\n",
    "            assert pred.shape[2] == self.output_step\n",
    "            assert y.shape[1] == self.output_step\n",
    "\n",
    "            _pred = pred.reshape(-1, self.output_step)\n",
    "            _y = y.reshape(-1, self.output_step)\n",
    "        \n",
    "            res = list()\n",
    "            for i in range(_pred.shape[0]):\n",
    "                res.append(r2_score(_y[i], _pred[i]))\n",
    "\n",
    "            res = np.array(res)\n",
    "        \n",
    "            return {\n",
    "                'mean_score': np.mean(res),\n",
    "                'max_score': res.max(),\n",
    "                'min_score': res.min(),\n",
    "                'logistic_score': model6_5.logistic_transform(np.mean(res))\n",
    "            }\n",
    "        except:\n",
    "            print(\"Array shapes don't match\")\n",
    "\n",
    "    def load_data(self, split=None, csv='data/^NDX_raw_data.csv'):\n",
    "        _data = pd.read_csv(csv)\n",
    "        _data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "        if not split:\n",
    "            return _data, pd.DataFrame()\n",
    "        else:\n",
    "            _data_backup = _data.iloc[split:]\n",
    "            _data = _data.iloc[:split]\n",
    "    \n",
    "            return _data, _data_backup\n",
    "\n",
    "    @staticmethod\n",
    "    ## Creating sequences\n",
    "    def create_dataset(dataset, time_step=1, output_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-output_step):\n",
    "            a = dataset[i:(i+time_step), 0]\n",
    "            b = dataset[(i+time_step):(i+time_step)+output_step, 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(b)\n",
    "    \n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    def savgol_smoothing(self, data_input):\n",
    "        from scipy.signal import savgol_filter\n",
    "\n",
    "        # Apply Savitzky-Golay filter\n",
    "        window_length = 17  # Window length (must be odd)\n",
    "        polyorder = 1      # Polynomial order\n",
    "        \n",
    "        return np.array([ savgol_filter(data_input[i], window_length, polyorder) for i in range(data_input.shape[0]) ])\n",
    "\n",
    "    def decompose_seasons(self, data_input):\n",
    "        from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "        decompositions = np.array([seasonal_decompose(data_input[i], model='additive', period=14) for i in range(data_input.shape[0])])\n",
    "        \n",
    "        return np.array([decompositions[i].seasonal for i in range(decompositions.shape[0])])\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        data_cropped = list()\n",
    "\n",
    "        # NO NEED TO CROP\n",
    "        for _data in data:\n",
    "            data_cropped.append(_data)\n",
    "        \n",
    "        data_cropped = np.array(data_cropped)\n",
    "        \n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        \n",
    "        scaler_data = list(MinMaxScaler() for i in range(data_cropped.shape[0]))\n",
    "        data_scaled = list()\n",
    "        \n",
    "        for i in range(data_cropped.shape[0]):\n",
    "            data_scaled.append(scaler_data[i].fit_transform(data_cropped[i].reshape(-1,1)))\n",
    "        \n",
    "        data_scaled = np.array(data_scaled)\n",
    "        \n",
    "        X_input = list()\n",
    "        y_input = list()\n",
    "        \n",
    "        for _data in data_scaled:\n",
    "            X_input.append(_data[:-self.y.shape[1]])\n",
    "            y_input.append(_data[-self.y.shape[1]:])\n",
    "        \n",
    "        X_input = np.array(X_input)\n",
    "        y_input = np.array(y_input)\n",
    "\n",
    "        return X_input, y_input, scaler_data\n",
    "\n",
    "    def create_model(self, checkpoint_path='model_weights_6_5_OOP', component=''):\n",
    "        import numpy as np\n",
    "        from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "        from tensorflow.keras import Input\n",
    "        from tensorflow.keras.models import Sequential, Model\n",
    "        from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Conv1D, AveragePooling1D, Flatten, Reshape, SimpleRNN, GRU, MaxPooling1D, concatenate\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        from tensorflow.keras.initializers import Zeros\n",
    "        \n",
    "        inputs = Input(shape=(self.input_period, self.num_features))\n",
    "        \n",
    "        model_cnn = Sequential([\n",
    "            Conv1D(filters=352, kernel_size=1, activation='relu'),\n",
    "            Conv1D(filters=352, kernel_size=1, activation='relu'),\n",
    "            MaxPooling1D(pool_size=3),\n",
    "            Dense(units=128),\n",
    "            Flatten()\n",
    "            #Dense(units=output_step),\n",
    "            #Reshape((output_step,1))\n",
    "        ])\n",
    "        \n",
    "        model_bilstm = Sequential([\n",
    "            Bidirectional(LSTM(units=384, return_sequences=True, activation='tanh', recurrent_activation='sigmoid')),\n",
    "            Dropout(0.2),\n",
    "            Flatten()\n",
    "            #Dense(units=output_step),\n",
    "            #Reshape((output_step,1))\n",
    "        ])\n",
    "        \n",
    "        model_bigru = Sequential([\n",
    "            Bidirectional(GRU(units=128, activation='tanh', return_sequences=True)),\n",
    "            Dropout(0.4),\n",
    "            Flatten()\n",
    "        ])\n",
    "        \n",
    "        model_multilayer_lstm = Sequential([\n",
    "            LSTM(units=64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'),\n",
    "            Dropout(0.2),\n",
    "            LSTM(units=64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'),\n",
    "            Flatten()\n",
    "        ])\n",
    "\n",
    "        output_cnn = model_cnn(inputs)\n",
    "        output_bilstm = model_bilstm(inputs)\n",
    "        output_bigru = model_bigru(inputs)\n",
    "        output_multilayer_lstm = model_multilayer_lstm(inputs)\n",
    "        \n",
    "        concatenated_outputs = concatenate([output_cnn, output_bilstm, output_bigru, output_multilayer_lstm])\n",
    "        \n",
    "        main_model = Sequential([\n",
    "            Input(shape=(concatenated_outputs.shape[1],)),\n",
    "            #Dense(units=output_step),\n",
    "            Dense(units=self.output_step, kernel_initializer=Zeros(), use_bias=False),\n",
    "            Reshape((self.output_step,1))\n",
    "        ])\n",
    "        \n",
    "        final_output = main_model(concatenated_outputs)\n",
    "        \n",
    "        functional_pipeline = Model(inputs=inputs, outputs=final_output)\n",
    "\n",
    "        # Compile the pipeline model\n",
    "        functional_pipeline.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "        \n",
    "        self.checkpoint = ModelCheckpoint(filepath=f'{checkpoint_path}_{self.ohlc}-{component}' + '/model_weights_epoch_{epoch:02d}.h5', \n",
    "                                    save_best_only=True, save_weights_only=True)\n",
    "        self.early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        return functional_pipeline\n",
    "\n",
    "    def generate_input_data(self, component, split=None, csv='data/^NDX_raw_data.csv'):\n",
    "        self.data, self.data_backup = self.load_data(split=split, csv=csv)\n",
    "        self.X, self.y = model6_5.create_dataset(self.data[[self.ohlc]].to_numpy(), time_step=self.period, output_step=self.output_step)\n",
    "        self.data_input = np.array([np.concatenate((self.X[i], self.y[i]), axis=0) for i in range(self.X.shape[0])])\n",
    "\n",
    "        if 'savgol' == component.lower():\n",
    "            self.feat_eng = self.savgol_smoothing(self.data_input)\n",
    "        if 'seasons' == component.lower():\n",
    "            self.feat_eng = self.decompose_seasons(self.data_input)\n",
    "\n",
    "        self.X_input, self.y_input, self.scalers = self.preprocess_data(self.feat_eng)\n",
    "\n",
    "        return self.X_input, self.y_input, self.scalers\n",
    "\n",
    "    def fit_model(self, component, split, checkpoint_path='model_weights_6_5_OOP'):\n",
    "        X_input, y_input, _ = self.generate_input_data(component=component, split=split)\n",
    "\n",
    "        # Train the pipeline model\n",
    "        history = self.model_dict[component].fit(X_input, y_input, epochs=150, batch_size=64, validation_split=0.2, callbacks=[self.checkpoint, self.early_stopping])\n",
    "\n",
    "        # Pickle the object and save it to a file\n",
    "        #import pickle\n",
    "        with open(f'{checkpoint_path}_{self.ohlc}-{component}/best_epoch.txt', 'w') as f:\n",
    "            best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "            f.write(f'best epoch: {best_epoch}')\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def load_epoch_weights(self, component, best_epoch=None, checkpoint_path='model_weights_6_5_OOP'):\n",
    "        # Load the weights of the model at the chosen epoch\n",
    "        self.model_dict[component].load_weights(f'{checkpoint_path}_{self.ohlc}-{component}/model_weights_epoch_{best_epoch:02d}.h5')\n",
    "        print('Backup: Weigths for the best epoch has been loaded.')\n",
    "\n",
    "    def predict_model(self, component, split=None, csv='data/^NDX_raw_data.csv'):\n",
    "        X_input, y_input, scalers = self.generate_input_data(component, split=split, csv=csv)\n",
    "        \n",
    "        predictions = list()\n",
    "        test_inputs = list()\n",
    "\n",
    "        for j in range(X_input.shape[0]):\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(j+1)\n",
    "\n",
    "            predictions.append(\n",
    "                scalers[j].inverse_transform(\n",
    "                    self.model_dict[component].predict(X_input[j].reshape(1, self.input_period, self.num_features), verbose=0)[0].reshape(1,self.output_step)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            test_inputs.append(\n",
    "                scalers[j].inverse_transform(\n",
    "                    y_input[j]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return np.array(predictions), np.array(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebc95549-b877-4992-bfca-91b557152f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_savgol_high = model6_5(\"savgol\", ohlc='high')\n",
    "model_seasons_high = model6_5(\"seasons\", ohlc='high')\n",
    "\n",
    "model_savgol_low = model6_5(\"savgol\", ohlc='low')\n",
    "model_seasons_low = model6_5(\"seasons\", ohlc='low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60f32446-6a95-4859-b34c-13fa5b36ed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup: Weigths for the best epoch has been loaded.\n",
      "Backup: Weigths for the best epoch has been loaded.\n",
      "Backup: Weigths for the best epoch has been loaded.\n",
      "Backup: Weigths for the best epoch has been loaded.\n"
     ]
    }
   ],
   "source": [
    "model_savgol_high.load_epoch_weights(\"savgol\", best_epoch=45)\n",
    "model_seasons_high.load_epoch_weights(\"seasons\", best_epoch=16)\n",
    "\n",
    "model_savgol_low.load_epoch_weights(\"savgol\", best_epoch=32)\n",
    "model_seasons_low.load_epoch_weights(\"seasons\", best_epoch=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef108f2-1fdd-4fa5-88c0-57ef71173154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "done\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "done\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "done\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pred_savgol_high, y_savgol_high = model_savgol_high.predict_model(\"savgol\", csv='data-china-from2018-test/000001.SS_raw_data.csv')\n",
    "print(\"done\")\n",
    "pred_seasons_high, y_seasons_high = model_seasons_high.predict_model(\"seasons\", csv='data-china-from2018-test/000001.SS_raw_data.csv')\n",
    "print(\"done\")\n",
    "\n",
    "pred_savgol_low, y_savgol_low = model_savgol_low.predict_model(\"savgol\", csv='data-china-from2018-test/000001.SS_raw_data.csv')\n",
    "print(\"done\")\n",
    "pred_seasons_low, y_seasons_low = model_seasons_low.predict_model(\"seasons\", csv='data-china-from2018-test/000001.SS_raw_data.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49676efe-4a7f-47ff-bafa-2ddf8f084917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2078520/1527985766.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  return round(1 / (1 + np.exp(-r2)),2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_score': -108.99353763865534, 'max_score': 0.9998553703092787, 'min_score': -105620.24572780391, 'logistic_score': 0.0}\n",
      "{'mean_score': 0.9997689064592966, 'max_score': 0.9999885967724471, 'min_score': 0.9973684142241515, 'logistic_score': 0.73}\n",
      "{'mean_score': -23261.468928716884, 'max_score': 0.9997198372720412, 'min_score': -30034704.984813962, 'logistic_score': 0.0}\n",
      "{'mean_score': 0.9998392844467581, 'max_score': 0.9999886117228687, 'min_score': 0.9972401469350262, 'logistic_score': 0.73}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "score_savgol_high = model_savgol_high.report_score((pred_savgol_high, y_savgol_high))\n",
    "score_seasons_high = model_seasons_high.report_score((pred_seasons_high, y_seasons_high))\n",
    "\n",
    "score_savgol_low = model_savgol_low.report_score((pred_savgol_low, y_savgol_low))\n",
    "score_seasons_low = model_seasons_low.report_score((pred_seasons_low, y_seasons_low))\n",
    "\n",
    "print(score_savgol_high)\n",
    "print(score_seasons_high)\n",
    "print(score_savgol_low)\n",
    "print(score_seasons_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4ce6d-7537-41ea-81c7-1efddb77987e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e40e016-9850-46a6-9dc4-995d5354d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_high_df = pd.DataFrame([\n",
    "    pred_savgol_high.reshape(-1, 7)[i] + 3*pred_seasons_high.reshape(-1, 7)[i] for i in range(pred_savgol_high.shape[0])\n",
    "], columns=[f'pred_high_{i}' for i in range(model_savgol_high.output_step)],\n",
    "index=model_savgol_high.data.index[model_savgol_high.period:-model_savgol_high.output_step])\n",
    "\n",
    "observation_low_df = pd.DataFrame([\n",
    "    pred_savgol_low.reshape(-1, 7)[i] + 3*pred_seasons_low.reshape(-1, 7)[i] for i in range(pred_savgol_low.shape[0])\n",
    "], columns=[f'pred_low_{i}' for i in range(model_savgol_low.output_step)],\n",
    "index=model_savgol_low.data.index[model_savgol_low.period:-model_savgol_low.output_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9b175e-317d-45d1-81ab-97aaffbef5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = pd.concat([ model_savgol_high.data, observation_high_df, observation_low_df ], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d205212c-f27f-459e-9a1b-160953a95508",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output.to_excel(f'data_w_pred_m6_5-oop/test_w_pred.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddce195-beb3-47b6-b0fc-0595c3199bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>pred_high_0</th>\n",
       "      <th>pred_high_1</th>\n",
       "      <th>pred_high_2</th>\n",
       "      <th>pred_high_3</th>\n",
       "      <th>pred_high_4</th>\n",
       "      <th>pred_high_5</th>\n",
       "      <th>pred_high_6</th>\n",
       "      <th>pred_low_0</th>\n",
       "      <th>pred_low_1</th>\n",
       "      <th>pred_low_2</th>\n",
       "      <th>pred_low_3</th>\n",
       "      <th>pred_low_4</th>\n",
       "      <th>pred_low_5</th>\n",
       "      <th>pred_low_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-01-02 00:00:00-05:00</td>\n",
       "      <td>1474.160034</td>\n",
       "      <td>1479.589966</td>\n",
       "      <td>1458.510010</td>\n",
       "      <td>1463.569946</td>\n",
       "      <td>1666780000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-01-05 00:00:00-05:00</td>\n",
       "      <td>1474.550049</td>\n",
       "      <td>1496.579956</td>\n",
       "      <td>1474.189941</td>\n",
       "      <td>1496.579956</td>\n",
       "      <td>2362910000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-01-06 00:00:00-05:00</td>\n",
       "      <td>1492.410034</td>\n",
       "      <td>1504.469971</td>\n",
       "      <td>1486.589966</td>\n",
       "      <td>1501.260010</td>\n",
       "      <td>2273220000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-01-07 00:00:00-05:00</td>\n",
       "      <td>1498.380005</td>\n",
       "      <td>1514.449951</td>\n",
       "      <td>1491.199951</td>\n",
       "      <td>1514.260010</td>\n",
       "      <td>2294280000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-01-08 00:00:00-05:00</td>\n",
       "      <td>1524.060059</td>\n",
       "      <td>1530.650024</td>\n",
       "      <td>1513.339966</td>\n",
       "      <td>1530.650024</td>\n",
       "      <td>2683950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>2023-12-22 00:00:00-05:00</td>\n",
       "      <td>16799.019531</td>\n",
       "      <td>16839.250000</td>\n",
       "      <td>16703.570312</td>\n",
       "      <td>16777.400391</td>\n",
       "      <td>4796600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>2023-12-26 00:00:00-05:00</td>\n",
       "      <td>16816.779297</td>\n",
       "      <td>16907.509766</td>\n",
       "      <td>16813.570312</td>\n",
       "      <td>16878.460938</td>\n",
       "      <td>6120600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>2023-12-27 00:00:00-05:00</td>\n",
       "      <td>16896.140625</td>\n",
       "      <td>16922.009766</td>\n",
       "      <td>16859.550781</td>\n",
       "      <td>16906.800781</td>\n",
       "      <td>7480170000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>2023-12-28 00:00:00-05:00</td>\n",
       "      <td>16963.519531</td>\n",
       "      <td>16969.169922</td>\n",
       "      <td>16891.320312</td>\n",
       "      <td>16898.470703</td>\n",
       "      <td>5090570000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>2023-12-29 00:00:00-05:00</td>\n",
       "      <td>16902.439453</td>\n",
       "      <td>16919.509766</td>\n",
       "      <td>16757.890625</td>\n",
       "      <td>16825.929688</td>\n",
       "      <td>5441060000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5033 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date          open          high           low  \\\n",
       "0     2004-01-02 00:00:00-05:00   1474.160034   1479.589966   1458.510010   \n",
       "1     2004-01-05 00:00:00-05:00   1474.550049   1496.579956   1474.189941   \n",
       "2     2004-01-06 00:00:00-05:00   1492.410034   1504.469971   1486.589966   \n",
       "3     2004-01-07 00:00:00-05:00   1498.380005   1514.449951   1491.199951   \n",
       "4     2004-01-08 00:00:00-05:00   1524.060059   1530.650024   1513.339966   \n",
       "...                         ...           ...           ...           ...   \n",
       "5028  2023-12-22 00:00:00-05:00  16799.019531  16839.250000  16703.570312   \n",
       "5029  2023-12-26 00:00:00-05:00  16816.779297  16907.509766  16813.570312   \n",
       "5030  2023-12-27 00:00:00-05:00  16896.140625  16922.009766  16859.550781   \n",
       "5031  2023-12-28 00:00:00-05:00  16963.519531  16969.169922  16891.320312   \n",
       "5032  2023-12-29 00:00:00-05:00  16902.439453  16919.509766  16757.890625   \n",
       "\n",
       "             close      volume  pred_high_0  pred_high_1  pred_high_2  \\\n",
       "0      1463.569946  1666780000          NaN          NaN          NaN   \n",
       "1      1496.579956  2362910000          NaN          NaN          NaN   \n",
       "2      1501.260010  2273220000          NaN          NaN          NaN   \n",
       "3      1514.260010  2294280000          NaN          NaN          NaN   \n",
       "4      1530.650024  2683950000          NaN          NaN          NaN   \n",
       "...            ...         ...          ...          ...          ...   \n",
       "5028  16777.400391  4796600000          NaN          NaN          NaN   \n",
       "5029  16878.460938  6120600000          NaN          NaN          NaN   \n",
       "5030  16906.800781  7480170000          NaN          NaN          NaN   \n",
       "5031  16898.470703  5090570000          NaN          NaN          NaN   \n",
       "5032  16825.929688  5441060000          NaN          NaN          NaN   \n",
       "\n",
       "      pred_high_3  pred_high_4  pred_high_5  pred_high_6  pred_low_0  \\\n",
       "0             NaN          NaN          NaN          NaN         NaN   \n",
       "1             NaN          NaN          NaN          NaN         NaN   \n",
       "2             NaN          NaN          NaN          NaN         NaN   \n",
       "3             NaN          NaN          NaN          NaN         NaN   \n",
       "4             NaN          NaN          NaN          NaN         NaN   \n",
       "...           ...          ...          ...          ...         ...   \n",
       "5028          NaN          NaN          NaN          NaN         NaN   \n",
       "5029          NaN          NaN          NaN          NaN         NaN   \n",
       "5030          NaN          NaN          NaN          NaN         NaN   \n",
       "5031          NaN          NaN          NaN          NaN         NaN   \n",
       "5032          NaN          NaN          NaN          NaN         NaN   \n",
       "\n",
       "      pred_low_1  pred_low_2  pred_low_3  pred_low_4  pred_low_5  pred_low_6  \n",
       "0            NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "1            NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "2            NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "3            NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "4            NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "...          ...         ...         ...         ...         ...         ...  \n",
       "5028         NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "5029         NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "5030         NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "5031         NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "5032         NaN         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[5033 rows x 20 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
