{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78094f3d-9814-4780-8886-b692af3ed902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and copied.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/^NDX_raw_data.csv')\n",
    "data.rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "data_backup = data.iloc[3524:]\n",
    "\n",
    "data = data.iloc[:3524]\n",
    "data_copy = data.copy()\n",
    "\n",
    "print('Data imported and copied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db92254e-9cc3-4333-a29d-6428c074cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Creating sequences\n",
    "def create_dataset(dataset, time_step=1, output_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-output_step):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        b = dataset[(i+time_step):(i+time_step)+output_step, 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(b)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "period = 60\n",
    "trend_period = 14\n",
    "num_features = 1\n",
    "input_period = 46\n",
    "output_step = 7\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9d8f90-ded5-447b-a745-1cd01a9f6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 09:15:01.954009: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 09:15:02.042044: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-27 09:15:02.042103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-27 09:15:02.045356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-27 09:15:02.059474: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 09:15:03.839851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-27 09:15:05.642445: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2024-05-27 09:15:05.694696: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2024-05-27 09:15:05.716151: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2024-05-27 09:15:05.752438: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2024-05-27 09:15:07.278890: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing the Model...', flush=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=units, input_shape=(input_period, num_features)),\n",
    "    #LSTM(units=units, return_sequences=True, input_shape=(input_period, num_features)),\n",
    "    #Dropout(0.2),\n",
    "    #LSTM(units=units, return_sequences=False),\n",
    "    #Dropout(0.2),\n",
    "    Dense(output_step)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a ModelCheckpoint callback to save weights at the end of each epoch\n",
    "checkpoint = ModelCheckpoint(filepath='model_weights_2/model_weights_epoch_{epoch:02d}.h5', \n",
    "                            save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Adding early stopping to prevent overfitting\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print('Model has been initialized.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef33120-6f01-4e2a-8087-9fcc20188cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup: Weigths for the best epoch has been loaded.\n"
     ]
    }
   ],
   "source": [
    "best_epoch_backup = 94\n",
    "\n",
    "# Load the weights of the model at the chosen epoch\n",
    "model.load_weights(f'model_weights_2/model_weights_epoch_{best_epoch_backup:02d}.h5')\n",
    "print('Backup: Weigths for the best epoch has been loaded.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578ce704-4cca-4913-b914-73bb20e4ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the bulk data...\n",
      "Bulk data has been imported.\n"
     ]
    }
   ],
   "source": [
    "print('Importing the bulk data...', flush=True)\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "directory = 'data-nasdaq-from2018-test'\n",
    "\n",
    "df_data = dict()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    symbol_pattern = re.match(r'([^_]+)_', file)\n",
    "    symbol = symbol_pattern.group(1)\n",
    "    df_data[symbol] = pd.read_csv(os.path.join(directory, file))\n",
    "    df_data[symbol].rename(columns={'Date': 'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "print('Bulk data has been imported.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4996f0-bb24-44d6-8eee-17ff52d90c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the data sets...\n",
      "Data sets has been created.\n"
     ]
    }
   ],
   "source": [
    "print('Creating the data sets...', flush=True)\n",
    "\n",
    "dict_X_test, dict_y_test = dict(), dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    X_test, y_test = create_dataset(df_data[symbol][['close']].to_numpy(), time_step=period, output_step=output_step)\n",
    "\n",
    "    dict_X_test[symbol] = X_test\n",
    "    dict_y_test[symbol] = y_test\n",
    "\n",
    "print('Data sets has been created.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "951e374c-e8af-491e-b8d5-9a759889f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_X_test_augmented = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    x_test_augmented = list()\n",
    "    \n",
    "    for i in range(dict_X_test[symbol].shape[0]):\n",
    "        _arr = np.append(dict_X_test[symbol][i], np.array([dict_X_test[symbol][i][-1] for _ in range(int(trend_period/2))]))[int(trend_period/2):]\n",
    "        x_test_augmented.append(_arr)\n",
    "\n",
    "    dict_X_test_augmented[symbol] = np.array(x_test_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aba385a5-f88d-44aa-a44b-42b844940dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing data...\n",
      "Decomposition has been applied.\n"
     ]
    }
   ],
   "source": [
    "print('Decomposing data...', flush=True)\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "dict_decompositions = dict()\n",
    "dict_trends = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    decompositions_test = np.array([seasonal_decompose(dict_X_test_augmented[symbol][i], model='additive', period=14) for i in range(dict_X_test_augmented[symbol].shape[0])])\n",
    "    trends_test = np.array([decompositions_test[i].trend for i in range(decompositions_test.shape[0])])\n",
    "\n",
    "    dict_decompositions[symbol] = decompositions_test\n",
    "    dict_trends[symbol] = trends_test\n",
    "\n",
    "print('Decomposition has been applied.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86e5855b-8b63-44ca-8bbc-d9826c6f5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n",
      "Data scaled.\n"
     ]
    }
   ],
   "source": [
    "print('Scaling data...', flush=True)\n",
    "\n",
    "dict_trends_dropna = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    trends_test_dropna = list()\n",
    "    \n",
    "    for trend in dict_trends[symbol]:\n",
    "        trends_test_dropna.append(trend[~np.isnan(trend)])\n",
    "\n",
    "    trends_test_dropna = np.array(trends_test_dropna)\n",
    "\n",
    "    dict_trends_dropna[symbol] = trends_test_dropna\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dict_scalers_trend = dict()\n",
    "dict_scalers_target = dict()\n",
    "dict_trends_scaled = dict()\n",
    "dict_targets_scaled = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    scaler_trend = list(MinMaxScaler() for i in range(dict_trends_dropna[symbol].shape[0]))\n",
    "    trends_test_scaled = list()\n",
    "\n",
    "    # we use target values only for comparison issue here\n",
    "    scaler_target = list(MinMaxScaler() for i in range(dict_y_test[symbol].shape[0]))\n",
    "    target_test_scaled = list()\n",
    "\n",
    "    for i in range(dict_trends_dropna[symbol].shape[0]):\n",
    "        trends_test_scaled.append(scaler_trend[i].fit_transform(dict_trends_dropna[symbol][i].reshape(-1,1)))\n",
    "\n",
    "    for j in range(dict_y_test[symbol].shape[0]):\n",
    "        target_test_scaled.append(scaler_target[j].fit_transform(dict_y_test[symbol][j].reshape(-1,1)))\n",
    "\n",
    "    trends_test_scaled = np.array(trends_test_scaled)\n",
    "    target_test_scaled = np.array(target_test_scaled)\n",
    "\n",
    "    dict_scalers_trend[symbol] = scaler_trend\n",
    "    dict_scalers_target[symbol] = scaler_target\n",
    "    dict_trends_scaled[symbol] = trends_test_scaled\n",
    "    dict_targets_scaled[symbol] = target_test_scaled\n",
    "\n",
    "print('Data scaled.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1b752-8fc7-4088-9205-d02196937b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe4c4d8eec0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "print('Predicting...', flush=True)\n",
    "\n",
    "dict_predictions = dict()\n",
    "\n",
    "from tqdm import tqdm\n",
    "for nb, symbol in tqdm(enumerate(df_data)):\n",
    "    trend_predictions = list()\n",
    "\n",
    "    #print(f'{nb+1} predicting for {symbol}', flush=True)\n",
    "    \n",
    "    for j in range(dict_trends_scaled[symbol].shape[0]):\n",
    "        #print(f'{j+1} for {symbol}')\n",
    "        trend_predictions.append(\n",
    "            dict_scalers_trend[symbol][j].inverse_transform(\n",
    "                model.predict(dict_trends_scaled[symbol][j].reshape(dict_trends_scaled[symbol][j].shape[1], input_period, num_features), verbose=0)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    trend_predictions = np.array(trend_predictions)\n",
    "\n",
    "    dict_predictions[symbol] = trend_predictions\n",
    "\n",
    "print('Predictions has been completed.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0459348b-8ff0-452b-a05e-a555780482ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing observations for data...\n",
      "Observations have been constructed.\n"
     ]
    }
   ],
   "source": [
    "print('Constructing observations for data...', flush=True)\n",
    "\n",
    "dict_observations = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    observation = pd.Series([\n",
    "        dict_predictions[symbol][i][0] for i in range(dict_predictions[symbol].shape[0])\n",
    "    ], index=df_data[symbol].index[output_step:-period])\n",
    "\n",
    "    dict_observations[symbol] = observation\n",
    "\n",
    "print('Observations have been constructed.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "221ce7d0-b117-4906-bcd6-22944779b627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to data...\n",
      "Noise to data has been added.\n"
     ]
    }
   ],
   "source": [
    "print('Adding noise to data...', flush=True)\n",
    "\n",
    "dict_std_dev_noise = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    noise_high, _ = create_dataset(df_data[symbol][['high']].to_numpy(), time_step=period, output_step=output_step)\n",
    "    noise_low, _ = create_dataset(df_data[symbol][['low']].to_numpy(), time_step=period, output_step=output_step)\n",
    "\n",
    "    std_dev_high = np.array([np.std(val) for val in noise_high])\n",
    "    std_dev_low = np.array([np.std(val) for val in noise_low])\n",
    "    \n",
    "    std_dev_noise = np.array([np.maximum(val_high, val_low) for val_high, val_low in zip(std_dev_high, std_dev_low)])\n",
    "\n",
    "    dict_std_dev_noise[symbol] = std_dev_noise\n",
    "\n",
    "print('Noise to data has been added.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edad6265-9b9d-4ece-8e72-f9fe195bef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence interval...\n",
      "Induced gap calculated.\n"
     ]
    }
   ],
   "source": [
    "print('Calculating confidence interval...', flush=True)\n",
    "\n",
    "dict_induced_gap = dict()\n",
    "\n",
    "for symbol in df_data:\n",
    "    induced_highs = np.array([dict_observations[symbol][i+7] + dict_std_dev_noise[symbol][i] for i in range(dict_std_dev_noise[symbol].shape[0])])\n",
    "    induced_lows = np.array([dict_observations[symbol][i+7] - dict_std_dev_noise[symbol][i] for i in range(dict_std_dev_noise[symbol].shape[0])])\n",
    "    \n",
    "    induced_highs = pd.DataFrame(induced_highs, index=df_data[symbol].index[7:-60])\n",
    "    induced_lows = pd.DataFrame(induced_lows, index=df_data[symbol].index[7:-60])\n",
    "\n",
    "    dict_induced_gap[symbol] = {'induced_high':induced_highs, 'induced_low':induced_lows}\n",
    "\n",
    "print('Induced gap calculated.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ebe56f-9e8c-4218-b539-872ba49590f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 done for 000063.SZ\n",
      "2 done for 000157.SZ\n",
      "3 done for 000001.SS\n",
      "4 done for 000002.SZ\n",
      "5 done for 000100.SZ\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data_output = df_data.copy()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for symbol in df_data:\n",
    "    count += 1\n",
    "\n",
    "    observation_df = pd.DataFrame([dict_observations[symbol][k+7] for k in range(dict_observations[symbol].shape[0])], columns=[f'pred_{i}' for i in range(output_step)], index=df_data[symbol].index[7:-60])\n",
    "    data_output[symbol] = pd.concat([data_output[symbol], observation_df], axis=1, join='inner')\n",
    "\n",
    "    noise_df = pd.DataFrame(dict_std_dev_noise[symbol], columns=['noise'], index=df_data[symbol].index[7:-60])\n",
    "    data_output[symbol] = pd.concat([data_output[symbol], noise_df], axis=1, join='inner')\n",
    "\n",
    "    for i in range(output_step):\n",
    "        data_output[symbol][f'pred_{i}'] = data_output[symbol][f'pred_{i}'].shift(53)\n",
    "        \n",
    "    #data_output[symbol]['pred'] = dict_observations[symbol]\n",
    "    #data_output[symbol]['induced_high'] = dict_induced_gap[symbol]['induced_high']\n",
    "    #data_output[symbol]['induced_low'] = dict_induced_gap[symbol]['induced_low']\n",
    "    \n",
    "    #data_output[symbol]['pred'] = data_output[symbol]['pred'].shift(46)\n",
    "    #data_output[symbol]['induced_high'] = data_output[symbol]['induced_high'].shift(46)\n",
    "    #data_output[symbol]['induced_low'] = data_output[symbol]['induced_low'].shift(46)\n",
    "\n",
    "    data_output[symbol].to_excel(f'data_w_pred_m5-nasdaq-test/{symbol}_w_pred_m5.xlsx')\n",
    "    \n",
    "    print(f'{count} done for {symbol}', flush=True)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Pickle the object and save it to a file\n",
    "with open('data_pred-nasdaq-bulk.pickle', 'wb') as f:\n",
    "    pickle.dump(dict_predictions, f)\n",
    "\n",
    "print('Done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdb84c-ec67-4c05-a71e-91f31d1fbf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53adff-e95c-4104-8a59-2552f09a2328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f936b7-cca6-499a-983f-23ac51bb3226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
